{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78f9a3c2",
   "metadata": {},
   "source": [
    "# Chrome Dino RL Training - DQN with CNN (PyTorch + GPU)\n",
    "\n",
    "**Hardware Configuration:**\n",
    "- GPU: RTX 3050 Mobile (4GB VRAM) ‚úÖ CUDA Enabled\n",
    "- Drivers: NVIDIA Studio Drivers\n",
    "- Framework: PyTorch 2.5.1 with CUDA 12.1\n",
    "\n",
    "**Model Architecture:**\n",
    "- Deep Q-Network (DQN) with Convolutional Neural Networks\n",
    "- Input: 80x80 grayscale game screenshots\n",
    "- Output: Q-values for 3 actions (run, jump, duck)\n",
    "\n",
    "**Training Strategy:**\n",
    "- Experience Replay Buffer (10,000 transitions)\n",
    "- Target Network (updated every 1,000 steps)\n",
    "- Epsilon-greedy exploration (1.0 ‚Üí 0.1 over 10,000 steps)\n",
    "- Batch size: 32 (optimized for 4GB VRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4f58054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "GPU device: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "CUDA version: 12.1\n",
      "GPU memory: 4.00 GB\n",
      "\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
    "from browser_dino_env import BrowserDinoEnv\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö† No GPU detected - training will use CPU\")\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadcbec6",
   "metadata": {},
   "source": [
    "## 2. GPU Configuration for RTX 3050 Mobile (4GB VRAM)\n",
    "\n",
    "PyTorch with CUDA 12.1 provides native GPU acceleration on Windows.\n",
    "Your RTX 3050 Mobile GPU is ready for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c38b400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "‚úì Total VRAM: 4.00 GB\n",
      "‚úì Allocated: 0.05 GB\n",
      "‚úì Cached: 0.07 GB\n",
      "‚úì cuDNN benchmark: True\n"
     ]
    }
   ],
   "source": [
    "# GPU memory management\n",
    "if torch.cuda.is_available():\n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Set memory fraction (use ~90% of VRAM, leave some for Chrome)\n",
    "    torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "    \n",
    "    # Enable cuDNN benchmarking for optimal performance\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # Print memory info\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"‚úì GPU: {props.name}\")\n",
    "    print(f\"‚úì Total VRAM: {props.total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"‚úì Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"‚úì Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"‚úì cuDNN benchmark: {torch.backends.cudnn.benchmark}\")\n",
    "else:\n",
    "    print(\"‚ö† No GPU - training will be slow on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2f46dc",
   "metadata": {},
   "source": [
    "## 3. DQN Network with CNN Architecture (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80c82b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q-Network with CNN for processing game screenshots.\n",
    "    \n",
    "    Architecture:\n",
    "    - Conv2D(32, 8x8, stride 4) ‚Üí ReLU\n",
    "    - Conv2D(64, 4x4, stride 2) ‚Üí ReLU\n",
    "    - Conv2D(64, 3x3, stride 1) ‚Üí ReLU\n",
    "    - Flatten\n",
    "    - Dense(512) ‚Üí ReLU\n",
    "    - Dense(action_size) ‚Üí Linear (Q-values)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape=(1, 80, 80), action_size=3):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # Calculate flattened size\n",
    "        def conv2d_size_out(size, kernel_size, stride):\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(80, 8, 4), 4, 2), 3, 1)\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(80, 8, 4), 4, 2), 3, 1)\n",
    "        linear_input_size = convw * convh * 64\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(linear_input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Normalize to [0, 1]\n",
    "        x = x.float() / 255.0\n",
    "        \n",
    "        # Conv layers with ReLU\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # FC layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    DQN Agent with experience replay and target network.\n",
    "    Optimized for 4GB VRAM with efficient memory usage.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_shape=(1, 80, 80),  # (channels, height, width)\n",
    "        action_size=3,             # run, jump, duck\n",
    "        learning_rate=0.00025,\n",
    "        gamma=0.99,                # Discount factor\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.1,\n",
    "        epsilon_decay_steps=10000,\n",
    "        batch_size=32,\n",
    "        memory_size=10000,\n",
    "        target_update_freq=1000\n",
    "    ):\n",
    "        self.state_shape = state_shape\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        \n",
    "        # Exploration parameters\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps\n",
    "        self.epsilon_decay_rate = (epsilon_start - epsilon_end) / epsilon_decay_steps\n",
    "        \n",
    "        # Training parameters\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.steps = 0\n",
    "        \n",
    "        # Experience replay buffer\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        \n",
    "        # Create Q-network and target network\n",
    "        self.q_network = DQN(state_shape, action_size).to(self.device)\n",
    "        self.target_network = DQN(state_shape, action_size).to(self.device)\n",
    "        self.update_target_network()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "        print(\"‚úì DQN Agent initialized\")\n",
    "        print(f\"  - State shape: {state_shape}\")\n",
    "        print(f\"  - Action size: {action_size}\")\n",
    "        print(f\"  - Batch size: {batch_size}\")\n",
    "        print(f\"  - Memory size: {memory_size}\")\n",
    "        print(f\"  - Device: {self.device}\")\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy weights from Q-network to target network\"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay buffer\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state, training=True):\n",
    "        \"\"\"\n",
    "        Choose action using epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current game state (80x80 grayscale image)\n",
    "            training: If True, use epsilon-greedy; if False, use greedy\n",
    "        \n",
    "        Returns:\n",
    "            action: Integer action (0=run, 1=jump, 2=duck)\n",
    "        \"\"\"\n",
    "        # Exploration: random action\n",
    "        if training and np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        \n",
    "        # Exploitation: best action from Q-network\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.q_network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"\n",
    "        Train on a batch of experiences from replay buffer.\n",
    "        \n",
    "        Returns:\n",
    "            loss: Training loss for monitoring\n",
    "        \"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0.0\n",
    "        \n",
    "        # Sample random batch\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states = torch.FloatTensor(np.array([exp[0] for exp in batch])).to(self.device)\n",
    "        actions = torch.LongTensor([exp[1] for exp in batch]).to(self.device)\n",
    "        rewards = torch.FloatTensor([exp[2] for exp in batch]).to(self.device)\n",
    "        next_states = torch.FloatTensor(np.array([exp[3] for exp in batch])).to(self.device)\n",
    "        dones = torch.FloatTensor([exp[4] for exp in batch]).to(self.device)\n",
    "        \n",
    "        # Get current Q-values\n",
    "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_network(next_states).max(1)[0]\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "        \n",
    "        # Compute loss and backpropagate\n",
    "        loss = self.loss_fn(current_q, target_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 10.0)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        \"\"\"Decay epsilon for exploration\"\"\"\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon -= self.epsilon_decay_rate\n",
    "            self.epsilon = max(self.epsilon_end, self.epsilon)\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save model weights\"\"\"\n",
    "        torch.save({\n",
    "            'q_network': self.q_network.state_dict(),\n",
    "            'target_network': self.target_network.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'steps': self.steps,\n",
    "            'epsilon': self.epsilon\n",
    "        }, filepath)\n",
    "        print(f\"‚úì Model saved to {filepath}\")\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        \"\"\"Load model weights\"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.q_network.load_state_dict(checkpoint['q_network'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_network'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.steps = checkpoint.get('steps', 0)\n",
    "        self.epsilon = checkpoint.get('epsilon', self.epsilon_end)\n",
    "        print(f\"‚úì Model loaded from {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e282be31",
   "metadata": {},
   "source": [
    "## 4. Create Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1dfa2338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì DQN Agent initialized\n",
      "  - State shape: (1, 80, 80)\n",
      "  - Action size: 3\n",
      "  - Batch size: 32\n",
      "  - Memory size: 10000\n",
      "  - Device: cuda\n",
      "\n",
      "üìä Network Architecture:\n",
      "DQN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=2304, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=3, bias=True)\n",
      ")\n",
      "\n",
      "üìä Model Statistics:\n",
      "  Total parameters: 1,253,539\n",
      "  Trainable parameters: 1,253,539\n",
      "  Approximate size: 4.78 MB\n",
      "  Estimated VRAM usage (with batch): ~14.35 MB\n",
      "  Safe for RTX 3050 4GB: ‚úì Yes\n",
      "\n",
      "üíæ GPU Memory after forward pass:\n",
      "  Allocated: 54.20 MB\n",
      "  Reserved: 70.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy agent to visualize architecture\n",
    "dummy_agent = DQNAgent()\n",
    "print(\"\\nüìä Network Architecture:\")\n",
    "print(dummy_agent.q_network)\n",
    "\n",
    "# Calculate parameters\n",
    "total_params = sum(p.numel() for p in dummy_agent.q_network.parameters())\n",
    "trainable_params = sum(p.numel() for p in dummy_agent.q_network.parameters() if p.requires_grad)\n",
    "param_size_mb = (total_params * 4) / (1024 ** 2)  # 4 bytes per float32\n",
    "\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Approximate size: {param_size_mb:.2f} MB\")\n",
    "print(f\"  Estimated VRAM usage (with batch): ~{param_size_mb * 3:.2f} MB\")\n",
    "print(f\"  Safe for RTX 3050 4GB: {'‚úì Yes' if param_size_mb * 3 < 3000 else '‚úó No'}\")\n",
    "\n",
    "# Check GPU memory if available\n",
    "if torch.cuda.is_available():\n",
    "    dummy_input = torch.randn(32, 1, 80, 80).to(device)\n",
    "    _ = dummy_agent.q_network(dummy_input)\n",
    "    print(f\"\\nüíæ GPU Memory after forward pass:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "del dummy_agent  # Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eec64c5",
   "metadata": {},
   "source": [
    "## 5. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56897a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Training Configuration:\n",
      "  num_episodes: 1000\n",
      "  max_steps_per_episode: 5000\n",
      "  learning_rate: 0.00025\n",
      "  gamma: 0.99\n",
      "  batch_size: 32\n",
      "  memory_size: 10000\n",
      "  target_update_freq: 1000\n",
      "  epsilon_start: 1.0\n",
      "  epsilon_end: 0.1\n",
      "  epsilon_decay_steps: 10000\n",
      "  log_interval: 10\n",
      "  save_interval: 50\n",
      "  plot_interval: 50\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "CONFIG = {\n",
    "    # Training\n",
    "    'num_episodes': 1000,\n",
    "    'max_steps_per_episode': 5000,\n",
    "    \n",
    "    # DQN parameters\n",
    "    'learning_rate': 0.00025,\n",
    "    'gamma': 0.99,\n",
    "    'batch_size': 32,\n",
    "    'memory_size': 10000,\n",
    "    'target_update_freq': 1000,\n",
    "    \n",
    "    # Exploration\n",
    "    'epsilon_start': 1.0,\n",
    "    'epsilon_end': 0.1,\n",
    "    'epsilon_decay_steps': 10000,\n",
    "    \n",
    "    # Logging\n",
    "    'log_interval': 10,        # Print stats every N episodes\n",
    "    'save_interval': 50,       # Save model every N episodes\n",
    "    'plot_interval': 50,       # Update plots every N episodes\n",
    "}\n",
    "\n",
    "# Create directories for saving\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "print(\"üìã Training Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf81264",
   "metadata": {},
   "source": [
    "## 7. Initialize Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c60bd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelDinoEnvs:\n",
    "    \"\"\"\n",
    "    Wrapper for running multiple Chrome Dino environments in parallel.\n",
    "    Each environment runs in its own browser window.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_envs=4):\n",
    "        self.num_envs = num_envs\n",
    "        self.envs = []\n",
    "        \n",
    "        print(f\"üöÄ Initializing {num_envs} parallel environments...\")\n",
    "        for i in range(num_envs):\n",
    "            try:\n",
    "                env = BrowserDinoEnv()\n",
    "                self.envs.append(env)\n",
    "                print(f\"  ‚úì Environment {i+1}/{num_envs} ready\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó Environment {i+1} failed: {e}\")\n",
    "                # Clean up on failure\n",
    "                for env in self.envs:\n",
    "                    env.close()\n",
    "                raise\n",
    "        \n",
    "        print(f\"‚úÖ All {num_envs} environments initialized!\")\n",
    "        \n",
    "        # Store observation/action space from first env\n",
    "        self.observation_space = self.envs[0].observation_space\n",
    "        self.action_space = self.envs[0].action_space\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset all environments and return initial states\"\"\"\n",
    "        states = []\n",
    "        infos = []\n",
    "        \n",
    "        for env in self.envs:\n",
    "            state, info = env.reset()\n",
    "            states.append(state)\n",
    "            infos.append(info)\n",
    "        \n",
    "        return np.array(states), infos\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Take actions in all environments.\n",
    "        \n",
    "        Args:\n",
    "            actions: List of actions (one per environment)\n",
    "        \n",
    "        Returns:\n",
    "            states: Array of next states\n",
    "            rewards: Array of rewards\n",
    "            terminateds: Array of terminated flags\n",
    "            truncateds: Array of truncated flags\n",
    "            infos: List of info dicts\n",
    "        \"\"\"\n",
    "        states = []\n",
    "        rewards = []\n",
    "        terminateds = []\n",
    "        truncateds = []\n",
    "        infos = []\n",
    "        \n",
    "        for env, action in zip(self.envs, actions):\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            states.append(state)\n",
    "            rewards.append(reward)\n",
    "            terminateds.append(terminated)\n",
    "            truncateds.append(truncated)\n",
    "            infos.append(info)\n",
    "        \n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(rewards),\n",
    "            np.array(terminateds),\n",
    "            np.array(truncateds),\n",
    "            infos\n",
    "        )\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close all environments\"\"\"\n",
    "        print(f\"\\nüîí Closing {self.num_envs} environments...\")\n",
    "        for i, env in enumerate(self.envs):\n",
    "            try:\n",
    "                env.close()\n",
    "                print(f\"  ‚úì Environment {i+1} closed\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó Environment {i+1} error: {e}\")\n",
    "        print(\"‚úì All environments closed\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_envs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300857b4",
   "metadata": {},
   "source": [
    "## 6. Parallel Environment Wrapper\n",
    "\n",
    "Training on 4 browser windows simultaneously will:\n",
    "- 4x faster experience collection\n",
    "- Better sample diversity\n",
    "- More stable training\n",
    "- ~2-4 hours for 1000 episodes (vs 8-15 hours single env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e80f089e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéÆ Initializing Chrome Dino environments...\n",
      "üöÄ Initializing 4 parallel environments...\n",
      "‚úì Ads and unnecessary content hidden\n",
      "  ‚úì Environment 1/4 ready\n",
      "‚úì Ads and unnecessary content hidden\n",
      "  ‚úì Environment 1/4 ready\n",
      "‚úì Ads and unnecessary content hidden\n",
      "  ‚úì Environment 2/4 ready\n",
      "‚úì Ads and unnecessary content hidden\n",
      "  ‚úì Environment 2/4 ready\n",
      "‚úì Ads and unnecessary content hidden\n",
      "  ‚úì Environment 3/4 ready\n",
      "‚úì Ads and unnecessary content hidden\n",
      "  ‚úì Environment 3/4 ready\n",
      "‚úì Ads and unnecessary content hidden\n",
      "  ‚úì Environment 4/4 ready\n",
      "‚úÖ All 4 environments initialized!\n",
      "‚úì Ads and unnecessary content hidden\n",
      "  ‚úì Environment 4/4 ready\n",
      "‚úÖ All 4 environments initialized!\n",
      "‚úì Environment(s) initialized\n",
      "  Observation space: Box(0, 255, (80, 80), uint8)\n",
      "  Action space: Discrete(2)\n",
      "  Parallel envs: 4\n",
      "\n",
      "ü§ñ Initializing DQN agent...\n",
      "‚úì DQN Agent initialized\n",
      "  - State shape: (1, 80, 80)\n",
      "  - Action size: 3\n",
      "  - Batch size: 32\n",
      "  - Memory size: 10000\n",
      "  - Device: cuda\n",
      "\n",
      "‚úÖ Ready to train on GPU with 4x parallel collection!\n",
      "‚úì Environment(s) initialized\n",
      "  Observation space: Box(0, 255, (80, 80), uint8)\n",
      "  Action space: Discrete(2)\n",
      "  Parallel envs: 4\n",
      "\n",
      "ü§ñ Initializing DQN agent...\n",
      "‚úì DQN Agent initialized\n",
      "  - State shape: (1, 80, 80)\n",
      "  - Action size: 3\n",
      "  - Batch size: 32\n",
      "  - Memory size: 10000\n",
      "  - Device: cuda\n",
      "\n",
      "‚úÖ Ready to train on GPU with 4x parallel collection!\n"
     ]
    }
   ],
   "source": [
    "# Choose number of parallel environments (1-4 recommended for 4GB VRAM)\n",
    "NUM_PARALLEL_ENVS = 4\n",
    "\n",
    "# Initialize parallel environments\n",
    "print(\"üéÆ Initializing Chrome Dino environments...\")\n",
    "if NUM_PARALLEL_ENVS > 1:\n",
    "    env = ParallelDinoEnvs(num_envs=NUM_PARALLEL_ENVS)\n",
    "else:\n",
    "    env = BrowserDinoEnv()\n",
    "    \n",
    "print(f\"‚úì Environment(s) initialized\")\n",
    "print(f\"  Observation space: {env.observation_space}\")\n",
    "print(f\"  Action space: {env.action_space}\")\n",
    "print(f\"  Parallel envs: {NUM_PARALLEL_ENVS}\")\n",
    "\n",
    "# Initialize agent\n",
    "print(\"\\nü§ñ Initializing DQN agent...\")\n",
    "agent = DQNAgent(\n",
    "    state_shape=(1, 80, 80),  # PyTorch uses (C, H, W) format\n",
    "    action_size=3,\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    gamma=CONFIG['gamma'],\n",
    "    epsilon_start=CONFIG['epsilon_start'],\n",
    "    epsilon_end=CONFIG['epsilon_end'],\n",
    "    epsilon_decay_steps=CONFIG['epsilon_decay_steps'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    memory_size=CONFIG['memory_size'],\n",
    "    target_update_freq=CONFIG['target_update_freq']\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Ready to train on GPU with {NUM_PARALLEL_ENVS}x parallel collection!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b236bb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üÜï Starting fresh training with corrected score retrieval\n",
      "   (Score now matches displayed value - expect 30-50 range initially)\n",
      "\n",
      "‚úÖ Agent ready with epsilon=1.000, steps=0\n"
     ]
    }
   ],
   "source": [
    "# Resume training configuration\n",
    "# ‚ö†Ô∏è IMPORTANT: Score retrieval was fixed! Old checkpoints have inflated scores (40x too high)\n",
    "# Set RESUME_TRAINING = False to start fresh with corrected scores\n",
    "RESUME_TRAINING = False  # Set to True to resume from checkpoint (only if checkpoint uses correct scores)\n",
    "CHECKPOINT_PATH = 'models/best_model.pth'  # or 'models/checkpoint_ep50.pth'\n",
    "\n",
    "# Initialize metric variables\n",
    "episode_rewards = []\n",
    "episode_scores = []\n",
    "episode_losses = []\n",
    "episode_lengths = []\n",
    "epsilon_history = []\n",
    "best_score = 0\n",
    "best_reward = float('-inf')\n",
    "\n",
    "if RESUME_TRAINING and os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"üîÑ Resuming training from {CHECKPOINT_PATH}\")\n",
    "    agent.load(CHECKPOINT_PATH)\n",
    "    \n",
    "    # Try to load previous metrics to continue plotting\n",
    "    try:\n",
    "        # Find latest metrics file\n",
    "        metric_files = sorted([f for f in os.listdir('logs') if f.startswith('metrics_ep') and f.endswith('.json')])\n",
    "        if metric_files:\n",
    "            latest_metrics = metric_files[-1]\n",
    "            with open(f'logs/{latest_metrics}', 'r') as f:\n",
    "                prev_metrics = json.load(f)\n",
    "                \n",
    "                # Load previous metrics\n",
    "                episode_rewards = prev_metrics.get('episode_rewards', [])\n",
    "                episode_scores = prev_metrics.get('episode_scores', [])\n",
    "                episode_losses = prev_metrics.get('episode_losses', [])\n",
    "                episode_lengths = prev_metrics.get('episode_lengths', [])\n",
    "                epsilon_history = prev_metrics.get('epsilon_history', [])\n",
    "                best_score = prev_metrics.get('best_score', 0)\n",
    "                best_reward = prev_metrics.get('best_reward', float('-inf'))\n",
    "                \n",
    "                # Adjust episode count to continue from where we left off\n",
    "                completed_episodes = prev_metrics.get('episode', 0)\n",
    "                print(f\"‚úì Loaded metrics from episode {completed_episodes}\")\n",
    "                print(f\"  Previous best score: {best_score}\")\n",
    "                print(f\"  Previous best reward: {best_reward:.2f}\")\n",
    "                print(f\"  Current epsilon: {agent.epsilon:.3f}\")\n",
    "                print(f\"  Total steps: {agent.steps}\")\n",
    "                print(f\"  Memory buffer: {len(agent.memory)} experiences\")\n",
    "                \n",
    "                # Update CONFIG to train for additional episodes\n",
    "                print(f\"\\nüìã Training will continue for {CONFIG['num_episodes']} more episodes\")\n",
    "                print(f\"   (Total episodes will be: {completed_episodes + CONFIG['num_episodes']})\")\n",
    "        else:\n",
    "            print(\"‚ö† No previous metrics found, starting with loaded weights only\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Could not load previous metrics: {e}\")\n",
    "        print(\"  Will continue with loaded model weights only\")\n",
    "else:\n",
    "    if RESUME_TRAINING:\n",
    "        print(f\"‚ö† Checkpoint not found: {CHECKPOINT_PATH}\")\n",
    "        print(\"üÜï Starting fresh training instead\")\n",
    "    else:\n",
    "        print(\"üÜï Starting fresh training with corrected score retrieval\")\n",
    "        print(\"   (Score now matches displayed value - expect 30-50 range initially)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Agent ready with epsilon={agent.epsilon:.3f}, steps={agent.steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da32ef1f",
   "metadata": {},
   "source": [
    "## 7.5. Optional: Resume Training from Checkpoint\n",
    "\n",
    "Set `RESUME_TRAINING = True` to continue from a saved model instead of starting from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40706413",
   "metadata": {},
   "source": [
    "## 7.6. IMPORTANT: Restart Environments After Code Changes\n",
    "\n",
    "‚ö†Ô∏è **If you just updated the score retrieval in `browser_dino_env.py`, you MUST restart the environments!**\n",
    "\n",
    "The old browser instances are still running with the old code. Run the cell below to close and reinitialize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fa42aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close old environments and reinitialize with updated code\n",
    "print(\"üîÑ Restarting environments to load updated code...\")\n",
    "\n",
    "# Close existing environments\n",
    "try:\n",
    "    env.close()\n",
    "    print(\"‚úì Old environments closed\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Reimport the module to get latest code changes\n",
    "import importlib\n",
    "import sys\n",
    "if 'browser_dino_env' in sys.modules:\n",
    "    importlib.reload(sys.modules['browser_dino_env'])\n",
    "    from browser_dino_env import BrowserDinoEnv\n",
    "    print(\"‚úì Module reloaded with latest changes\")\n",
    "\n",
    "# Reinitialize environments\n",
    "print(f\"\\nüéÆ Reinitializing {NUM_PARALLEL_ENVS} environments...\")\n",
    "if NUM_PARALLEL_ENVS > 1:\n",
    "    env = ParallelDinoEnvs(num_envs=NUM_PARALLEL_ENVS)\n",
    "else:\n",
    "    env = BrowserDinoEnv()\n",
    "\n",
    "print(f\"\\n‚úÖ Environments restarted with corrected score retrieval!\")\n",
    "print(\"   Scores should now show realistic values (30-50 range initially)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684f3e3c",
   "metadata": {},
   "source": [
    "## 8. Training Loop (Parallel)\n",
    "\n",
    "Training will continue from loaded checkpoint if `RESUME_TRAINING = True` was set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a8a097b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training on GPU with 4x parallel environments...\n",
      "üìù Progress will be saved to logs/training_progress.txt\n",
      "================================================================================\n",
      "‚úì Model saved to models/best_model.pth                                                                                  \n",
      "üèÜ New best score: 1927\n",
      "Episode 1 complete | Score: 1906 | Steps: 6 | Reward: 78.10\n",
      "‚úì Model saved to models/best_model.pth                                                                                  \n",
      "üèÜ New best score: 1927\n",
      "Episode 1 complete | Score: 1906 | Steps: 6 | Reward: 78.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IC1807\\AppData\\Local\\Temp\\ipykernel_2768\\3717846263.py:153: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  dones = torch.FloatTensor([exp[4] for exp in batch]).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model saved to models/best_model.pth                                                                                  \n",
      "üèÜ New best score: 2050\n",
      "Episode 2 complete | Score: 1955 | Steps: 5 | Reward: 126.30\n",
      "Episode 3 complete | Score: 1815 | Steps: 6 | Reward: 109.15                                                            \n",
      "Episode 3 complete | Score: 1815 | Steps: 6 | Reward: 109.15                                                            \n",
      "Episode 4 complete | Score: 1856 | Steps: 5 | Reward: 125.88                                                            \n",
      "Episode 4 complete | Score: 1856 | Steps: 5 | Reward: 125.88                                                            \n",
      "Episode 5 complete | Score: 1902 | Steps: 4 | Reward: 78.58                                                             \n",
      "Episode 5 complete | Score: 1902 | Steps: 4 | Reward: 78.58                                                             \n",
      "‚úì Model saved to models/best_model.pth                                                                                  \n",
      "üèÜ New best score: 3159\n",
      "Episode 6 complete | Score: 2196 | Steps: 7 | Reward: 153.50\n",
      "‚úì Model saved to models/best_model.pth                                                                                  \n",
      "üèÜ New best score: 3159\n",
      "Episode 6 complete | Score: 2196 | Steps: 7 | Reward: 153.50\n",
      "Episode 7 complete | Score: 1853 | Steps: 5 | Reward: 121.20                                                            \n",
      "Episode 7 complete | Score: 1853 | Steps: 5 | Reward: 121.20                                                            \n",
      "Episode 8 complete | Score: 1956 | Steps: 4 | Reward: 81.68                                                             \n",
      "Episode 8 complete | Score: 1956 | Steps: 4 | Reward: 81.68                                                             \n",
      "Episode 9 complete | Score: 1898 | Steps: 3 | Reward: 75.65                                                             \n",
      "Episode 9 complete | Score: 1898 | Steps: 3 | Reward: 75.65                                                             \n",
      "                                                                                                                        \n",
      "Episode 10/1000 (4x parallel)\n",
      "  Avg Reward: 98.49 | Avg Score: 1923 | Avg Steps: 5\n",
      "  Loss: 1024.7478 | Epsilon: 0.983 | Memory: 194/10000 | GPU: 49MB\n",
      "  Best Score: 3159 | Best Reward: 153.50\n",
      "  Improvement (last 10 eps): Reward +0.00 | Score +0.00\n",
      "--------------------------------------------------------------------------------\n",
      "                                                                                                                        \n",
      "Episode 10/1000 (4x parallel)\n",
      "  Avg Reward: 98.49 | Avg Score: 1923 | Avg Steps: 5\n",
      "  Loss: 1024.7478 | Epsilon: 0.983 | Memory: 194/10000 | GPU: 49MB\n",
      "  Best Score: 3159 | Best Reward: 153.50\n",
      "  Improvement (last 10 eps): Reward +0.00 | Score +0.00\n",
      "--------------------------------------------------------------------------------\n",
      "Episode 11 complete | Score: 1948 | Steps: 6 | Reward: 86.98                                                            \n",
      "Episode 11 complete | Score: 1948 | Steps: 6 | Reward: 86.98                                                            \n",
      "Episode 12 complete | Score: 1848 | Steps: 6 | Reward: 167.03                                                           \n",
      "Episode 12 complete | Score: 1848 | Steps: 6 | Reward: 167.03                                                           \n",
      "Episode 13 complete | Score: 1852 | Steps: 5 | Reward: 121.60                                                           \n",
      "Episode 13 complete | Score: 1852 | Steps: 5 | Reward: 121.60                                                           \n",
      "Episode 14 complete | Score: 1921 | Steps: 4 | Reward: 79.73                                                            \n",
      "Episode 14 complete | Score: 1921 | Steps: 4 | Reward: 79.73                                                            \n",
      "Episode 15 complete | Score: 1910 | Steps: 4 | Reward: 77.58 | Œî Reward: +1.49, Œî Score: +4.45                          \n",
      "Episode 15 complete | Score: 1910 | Steps: 4 | Reward: 77.58 | Œî Reward: +1.49, Œî Score: +4.45                          \n",
      "Episode 16 complete | Score: 1880 | Steps: 5 | Reward: 124.35                                                           \n",
      "Episode 16 complete | Score: 1880 | Steps: 5 | Reward: 124.35                                                           \n",
      "Episode 17 complete | Score: 1927 | Steps: 4 | Reward: 80.35                                                            \n",
      "Episode 17 complete | Score: 1927 | Steps: 4 | Reward: 80.35                                                            \n",
      "Episode 18 complete | Score: 1903 | Steps: 4 | Reward: 82.50                                                            \n",
      "Episode 18 complete | Score: 1903 | Steps: 4 | Reward: 82.50                                                            \n",
      "\n",
      "‚ö† Training interrupted by user\n",
      "Saving current progress...\n",
      "‚úì Model saved to models/interrupted_ep19.pth\n",
      "\n",
      "üèÅ Training complete!\n",
      "  Total episodes: 18\n",
      "  Best score: 3159\n",
      "  Best reward: 167.03\n",
      "  Parallel speedup: ~4x\n",
      "üìù Full training progress saved to logs/training_progress.txt\n",
      "  Final GPU memory: 49MB\n",
      "\n",
      "üîí Closing 4 environments...\n",
      "\n",
      "‚ö† Training interrupted by user\n",
      "Saving current progress...\n",
      "‚úì Model saved to models/interrupted_ep19.pth\n",
      "\n",
      "üèÅ Training complete!\n",
      "  Total episodes: 18\n",
      "  Best score: 3159\n",
      "  Best reward: 167.03\n",
      "  Parallel speedup: ~4x\n",
      "üìù Full training progress saved to logs/training_progress.txt\n",
      "  Final GPU memory: 49MB\n",
      "\n",
      "üîí Closing 4 environments...\n",
      "  ‚úì Environment 1 closed\n",
      "  ‚úì Environment 1 closed\n",
      "  ‚úì Environment 2 closed\n",
      "  ‚úì Environment 2 closed\n",
      "  ‚úì Environment 3 closed\n",
      "  ‚úì Environment 3 closed\n",
      "  ‚úì Environment 4 closed\n",
      "‚úì All environments closed\n",
      "‚úì Browser(s) closed\n",
      "  ‚úì Environment 4 closed\n",
      "‚úì All environments closed\n",
      "‚úì Browser(s) closed\n"
     ]
    }
   ],
   "source": [
    "# Parallel environment tracking\n",
    "num_envs = NUM_PARALLEL_ENVS if NUM_PARALLEL_ENVS > 1 else 1\n",
    "\n",
    "# Calculate starting episode (for resume)\n",
    "start_episode = len(episode_rewards)\n",
    "\n",
    "# Progress tracking file\n",
    "progress_file = 'logs/training_progress.txt'\n",
    "with open(progress_file, 'a') as f:\n",
    "    f.write(f\"\\n{'='*80}\\n\")\n",
    "    f.write(f\"Training session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Starting from episode: {start_episode + 1}\\n\")\n",
    "    f.write(f\"Parallel environments: {num_envs}\\n\")\n",
    "    f.write(f\"{'='*80}\\n\\n\")\n",
    "\n",
    "print(f\"üöÄ Starting training on GPU with {num_envs}x parallel environments...\")\n",
    "if start_episode > 0:\n",
    "    print(f\"üìç Resuming from episode {start_episode + 1}\")\n",
    "print(f\"üìù Progress will be saved to {progress_file}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    for episode in range(start_episode, start_episode + CONFIG['num_episodes']):\n",
    "        # Reset environment(s)\n",
    "        if num_envs > 1:\n",
    "            states, infos = env.reset()\n",
    "            # Convert to PyTorch format: (N, H, W) -> (N, C, H, W)\n",
    "            states = np.expand_dims(states, axis=1)\n",
    "        else:\n",
    "            state, info = env.reset()\n",
    "            states = np.expand_dims(np.expand_dims(state, axis=0), axis=0)\n",
    "            infos = [info]\n",
    "        \n",
    "        episode_rewards_parallel = [0] * num_envs\n",
    "        episode_losses_parallel = []\n",
    "        dones = [False] * num_envs\n",
    "        steps = [0] * num_envs\n",
    "        \n",
    "        # Real-time progress tracking\n",
    "        step_count = 0\n",
    "        \n",
    "        # Episode loop\n",
    "        while not all(dones):\n",
    "            # Select actions for all environments\n",
    "            actions = []\n",
    "            for i in range(num_envs):\n",
    "                if not dones[i]:\n",
    "                    action = agent.act(states[i], training=True)\n",
    "                    actions.append(action)\n",
    "                else:\n",
    "                    actions.append(0)  # Dummy action for done envs\n",
    "            \n",
    "            # Take steps in all environments\n",
    "            if num_envs > 1:\n",
    "                next_states, rewards, terminateds, truncateds, new_infos = env.step(actions)\n",
    "                next_states = np.expand_dims(next_states, axis=1)\n",
    "            else:\n",
    "                next_state, reward, terminated, truncated, new_info = env.step(actions[0])\n",
    "                next_states = np.expand_dims(np.expand_dims(next_state, axis=0), axis=0)\n",
    "                rewards = np.array([reward])\n",
    "                terminateds = np.array([terminated])\n",
    "                truncateds = np.array([truncated])\n",
    "                new_infos = [new_info]\n",
    "            \n",
    "            # Process each environment\n",
    "            for i in range(num_envs):\n",
    "                if not dones[i]:\n",
    "                    done = terminateds[i] or truncateds[i]\n",
    "                    \n",
    "                    # Store experience\n",
    "                    agent.remember(states[i], actions[i], rewards[i], next_states[i], done)\n",
    "                    \n",
    "                    # Train agent\n",
    "                    loss = agent.replay()\n",
    "                    if loss > 0:\n",
    "                        episode_losses_parallel.append(loss)\n",
    "                    \n",
    "                    # Update metrics\n",
    "                    episode_rewards_parallel[i] += rewards[i]\n",
    "                    steps[i] += 1\n",
    "                    agent.steps += 1\n",
    "                    \n",
    "                    # Update target network\n",
    "                    if agent.steps % agent.target_update_freq == 0:\n",
    "                        agent.update_target_network()\n",
    "                    \n",
    "                    # Decay epsilon\n",
    "                    agent.update_epsilon()\n",
    "                    \n",
    "                    # Mark as done\n",
    "                    if done:\n",
    "                        dones[i] = True\n",
    "                        infos[i] = new_infos[i]\n",
    "            \n",
    "            # Update states\n",
    "            states = next_states\n",
    "            step_count += 1\n",
    "            \n",
    "            # Real-time progress update (every 50 steps)\n",
    "            if step_count % 50 == 0:\n",
    "                active_envs = sum(1 for d in dones if not d)\n",
    "                avg_steps = np.mean([s for i, s in enumerate(steps) if not dones[i]]) if active_envs > 0 else np.mean(steps)\n",
    "                current_scores = [info.get('score', 0) for i, info in enumerate(infos) if not dones[i]]\n",
    "                avg_score = np.mean(current_scores) if current_scores else 0\n",
    "                \n",
    "                print(f\"  Episode {episode + 1} | Step {step_count} | \"\n",
    "                      f\"Active: {active_envs}/{num_envs} | \"\n",
    "                      f\"Avg Steps: {avg_steps:.0f} | \"\n",
    "                      f\"Current Score: {avg_score:.0f} | \"\n",
    "                      f\"Œµ: {agent.epsilon:.3f}\", end='\\r')\n",
    "            \n",
    "            # Break if max steps reached for all envs\n",
    "            if all(s >= CONFIG['max_steps_per_episode'] for s in steps):\n",
    "                break\n",
    "        \n",
    "        # Clear the progress line\n",
    "        print(\" \" * 120, end='\\r')\n",
    "        \n",
    "        # Episode complete - record metrics (average across parallel envs)\n",
    "        avg_episode_reward = np.mean(episode_rewards_parallel)\n",
    "        avg_episode_score = np.mean([info.get('score', 0) for info in infos])\n",
    "        avg_episode_length = np.mean(steps)\n",
    "        avg_episode_loss = np.mean(episode_losses_parallel) if episode_losses_parallel else 0\n",
    "        \n",
    "        episode_rewards.append(avg_episode_reward)\n",
    "        episode_scores.append(avg_episode_score)\n",
    "        episode_lengths.append(avg_episode_length)\n",
    "        episode_losses.append(avg_episode_loss)\n",
    "        epsilon_history.append(agent.epsilon)\n",
    "        \n",
    "        # Calculate improvement metrics\n",
    "        recent_window = 10\n",
    "        if len(episode_rewards) >= recent_window:\n",
    "            recent_avg_reward = np.mean(episode_rewards[-recent_window:])\n",
    "            prev_avg_reward = np.mean(episode_rewards[-recent_window*2:-recent_window]) if len(episode_rewards) >= recent_window*2 else np.mean(episode_rewards[:recent_window])\n",
    "            reward_improvement = recent_avg_reward - prev_avg_reward\n",
    "            \n",
    "            recent_avg_score = np.mean(episode_scores[-recent_window:])\n",
    "            prev_avg_score = np.mean(episode_scores[-recent_window*2:-recent_window]) if len(episode_scores) >= recent_window*2 else np.mean(episode_scores[:recent_window])\n",
    "            score_improvement = recent_avg_score - prev_avg_score\n",
    "        else:\n",
    "            reward_improvement = 0\n",
    "            score_improvement = 0\n",
    "        \n",
    "        # Update best performance\n",
    "        max_score = max(info.get('score', 0) for info in infos)\n",
    "        is_new_best = False\n",
    "        if max_score > best_score:\n",
    "            best_score = max_score\n",
    "            agent.save(f'models/best_model.pth')\n",
    "            is_new_best = True\n",
    "            print(f\"üèÜ New best score: {best_score:.0f}\")\n",
    "        \n",
    "        if avg_episode_reward > best_reward:\n",
    "            best_reward = avg_episode_reward\n",
    "        \n",
    "        # Save detailed progress to file\n",
    "        with open(progress_file, 'a') as f:\n",
    "            f.write(f\"Episode {episode + 1}/{start_episode + CONFIG['num_episodes']} - {datetime.now().strftime('%H:%M:%S')}\\n\")\n",
    "            f.write(f\"  Score: {avg_episode_score:.2f} (Best: {best_score:.0f}{'  üèÜ NEW BEST!' if is_new_best else ''})\\n\")\n",
    "            f.write(f\"  Reward: {avg_episode_reward:.2f} (Best: {best_reward:.2f})\\n\")\n",
    "            f.write(f\"  Steps: {avg_episode_length:.0f}\\n\")\n",
    "            f.write(f\"  Loss: {avg_episode_loss:.6f}\\n\")\n",
    "            f.write(f\"  Epsilon: {agent.epsilon:.4f}\\n\")\n",
    "            f.write(f\"  Total training steps: {agent.steps}\\n\")\n",
    "            f.write(f\"  Memory buffer: {len(agent.memory)}/{CONFIG['memory_size']}\\n\")\n",
    "            if len(episode_rewards) >= recent_window:\n",
    "                f.write(f\"  Improvement (last {recent_window} eps):\\n\")\n",
    "                f.write(f\"    Reward: {reward_improvement:+.2f}\\n\")\n",
    "                f.write(f\"    Score: {score_improvement:+.2f}\\n\")\n",
    "            f.write(f\"\\n\")\n",
    "        \n",
    "        # Logging\n",
    "        if (episode + 1) % CONFIG['log_interval'] == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-CONFIG['log_interval']:])\n",
    "            avg_score = np.mean(episode_scores[-CONFIG['log_interval']:])\n",
    "            avg_loss = np.mean(episode_losses[-CONFIG['log_interval']:])\n",
    "            avg_length = np.mean(episode_lengths[-CONFIG['log_interval']:])\n",
    "            \n",
    "            # GPU memory info\n",
    "            gpu_mem = \"\"\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_mem = f\" | GPU: {torch.cuda.memory_allocated(0) / 1024**2:.0f}MB\"\n",
    "            \n",
    "            print(f\"\\nEpisode {episode + 1}/{start_episode + CONFIG['num_episodes']} ({num_envs}x parallel)\")\n",
    "            print(f\"  Avg Reward: {avg_reward:.2f} | Avg Score: {avg_score:.0f} | \"\n",
    "                  f\"Avg Steps: {avg_length:.0f}\")\n",
    "            print(f\"  Loss: {avg_loss:.4f} | Epsilon: {agent.epsilon:.3f} | \"\n",
    "                  f\"Memory: {len(agent.memory)}/{CONFIG['memory_size']}{gpu_mem}\")\n",
    "            print(f\"  Best Score: {best_score:.0f} | Best Reward: {best_reward:.2f}\")\n",
    "            \n",
    "            if len(episode_rewards) >= recent_window:\n",
    "                print(f\"  Improvement (last {recent_window} eps): \"\n",
    "                      f\"Reward {reward_improvement:+.2f} | Score {score_improvement:+.2f}\")\n",
    "            print(\"-\" * 80)\n",
    "        else:\n",
    "            # Brief update for non-logged episodes\n",
    "            improvement_str = \"\"\n",
    "            if len(episode_rewards) >= recent_window and (episode + 1) % 5 == 0:\n",
    "                improvement_str = f\" | Œî Reward: {reward_improvement:+.2f}, Œî Score: {score_improvement:+.2f}\"\n",
    "            \n",
    "            print(f\"Episode {episode + 1} complete | Score: {avg_episode_score:.0f} | \"\n",
    "                  f\"Steps: {avg_episode_length:.0f} | \"\n",
    "                  f\"Reward: {avg_episode_reward:.2f}{improvement_str}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (episode + 1) % CONFIG['save_interval'] == 0:\n",
    "            agent.save(f'models/checkpoint_ep{episode + 1}.pth')\n",
    "            \n",
    "            # Save metrics with improvement tracking\n",
    "            metrics = {\n",
    "                'episode': episode + 1,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'num_parallel_envs': num_envs,\n",
    "                'episode_rewards': episode_rewards,\n",
    "                'episode_scores': episode_scores,\n",
    "                'episode_losses': episode_losses,\n",
    "                'episode_lengths': episode_lengths,\n",
    "                'epsilon_history': epsilon_history,\n",
    "                'best_score': best_score,\n",
    "                'best_reward': best_reward,\n",
    "                'total_steps': agent.steps,\n",
    "                'memory_size': len(agent.memory),\n",
    "                'config': CONFIG\n",
    "            }\n",
    "            \n",
    "            # Add improvement metrics\n",
    "            if len(episode_rewards) >= recent_window:\n",
    "                metrics['recent_improvement'] = {\n",
    "                    'window_size': recent_window,\n",
    "                    'reward_improvement': float(reward_improvement),\n",
    "                    'score_improvement': float(score_improvement),\n",
    "                    'recent_avg_reward': float(recent_avg_reward),\n",
    "                    'recent_avg_score': float(recent_avg_score)\n",
    "                }\n",
    "            \n",
    "            with open(f'logs/metrics_ep{episode + 1}.json', 'w') as f:\n",
    "                json.dump(metrics, f, indent=2)\n",
    "            \n",
    "            print(f\"üíæ Checkpoint saved: models/checkpoint_ep{episode + 1}.pth\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö† Training interrupted by user\")\n",
    "    print(\"Saving current progress...\")\n",
    "    agent.save(f'models/interrupted_ep{episode + 1}.pth')\n",
    "    \n",
    "    with open(progress_file, 'a') as f:\n",
    "        f.write(f\"\\n‚ö† Training interrupted at episode {episode + 1}\\n\")\n",
    "        f.write(f\"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "finally:\n",
    "    print(\"\\nüèÅ Training complete!\")\n",
    "    print(f\"  Total episodes: {len(episode_rewards)}\")\n",
    "    print(f\"  Best score: {best_score}\")\n",
    "    print(f\"  Best reward: {best_reward:.2f}\")\n",
    "    print(f\"  Parallel speedup: ~{num_envs}x\")\n",
    "    \n",
    "    # Save final summary to progress file\n",
    "    with open(progress_file, 'a') as f:\n",
    "        f.write(f\"\\n{'='*80}\\n\")\n",
    "        f.write(f\"Training session ended: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Total episodes completed: {len(episode_rewards)}\\n\")\n",
    "        f.write(f\"Best score achieved: {best_score:.0f}\\n\")\n",
    "        f.write(f\"Best reward achieved: {best_reward:.2f}\\n\")\n",
    "        f.write(f\"Total training steps: {agent.steps}\\n\")\n",
    "        f.write(f\"Final epsilon: {agent.epsilon:.4f}\\n\")\n",
    "        f.write(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"üìù Full training progress saved to {progress_file}\")\n",
    "    \n",
    "    # GPU memory cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  Final GPU memory: {torch.cuda.memory_allocated(0) / 1024**2:.0f}MB\")\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Close environment(s)\n",
    "    env.close()\n",
    "    print(\"‚úì Browser(s) closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a49e4",
   "metadata": {},
   "source": [
    "## 9. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55176299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('DQN Training Metrics', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Rewards\n",
    "axes[0, 0].plot(episode_rewards, alpha=0.3, label='Episode Reward')\n",
    "axes[0, 0].plot(np.convolve(episode_rewards, np.ones(50)/50, mode='valid'), \n",
    "                label='Moving Avg (50)', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Total Reward')\n",
    "axes[0, 0].set_title('Episode Rewards')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scores\n",
    "axes[0, 1].plot(episode_scores, alpha=0.3, label='Episode Score')\n",
    "axes[0, 1].plot(np.convolve(episode_scores, np.ones(50)/50, mode='valid'),\n",
    "                label='Moving Avg (50)', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Score (Distance)')\n",
    "axes[0, 1].set_title('Game Scores')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1, 0].plot(episode_losses, alpha=0.6)\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].set_title('Training Loss')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Epsilon\n",
    "axes[1, 1].plot(epsilon_history, color='orange')\n",
    "axes[1, 1].set_xlabel('Episode')\n",
    "axes[1, 1].set_ylabel('Epsilon')\n",
    "axes[1, 1].set_title('Exploration Rate (Epsilon)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/training_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Training plots saved to plots/training_metrics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5bf118",
   "metadata": {},
   "source": [
    "## 10. Test Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d9889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the best model\n",
    "print(\"üéÆ Testing trained agent...\\n\")\n",
    "\n",
    "# Load best model\n",
    "agent.load('models/best_model.pth')\n",
    "agent.epsilon = 0.0  # No exploration during testing\n",
    "\n",
    "# Run test episodes\n",
    "num_test_episodes = 5\n",
    "test_scores = []\n",
    "test_rewards = []\n",
    "\n",
    "env = BrowserDinoEnv()\n",
    "\n",
    "for episode in range(num_test_episodes):\n",
    "    state, info = env.reset()\n",
    "    state = np.expand_dims(state, axis=0)  # Add channel dimension\n",
    "    \n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "    \n",
    "    while not done and step < 5000:\n",
    "        action = agent.act(state, training=False)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        state = np.expand_dims(next_state, axis=0)\n",
    "        episode_reward += reward\n",
    "        step += 1\n",
    "    \n",
    "    test_scores.append(info.get('score', 0))\n",
    "    test_rewards.append(episode_reward)\n",
    "    \n",
    "    print(f\"Test Episode {episode + 1}: Score={info.get('score', 0):.0f}, \"\n",
    "          f\"Reward={episode_reward:.2f}, Steps={step}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\nüìä Test Results:\")\n",
    "print(f\"  Average Score: {np.mean(test_scores):.0f} ¬± {np.std(test_scores):.0f}\")\n",
    "print(f\"  Average Reward: {np.mean(test_rewards):.2f} ¬± {np.std(test_rewards):.2f}\")\n",
    "print(f\"  Best Score: {max(test_scores):.0f}\")\n",
    "print(f\"  Worst Score: {min(test_scores):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9841c5b2",
   "metadata": {},
   "source": [
    "## 11. Save Final Model and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cfb066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final summary\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "summary = {\n",
    "    'timestamp': timestamp,\n",
    "    'hardware': {\n",
    "        'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',\n",
    "        'vram': f\"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f}GB\" if torch.cuda.is_available() else 'N/A',\n",
    "        'pytorch_version': torch.__version__,\n",
    "        'cuda_version': torch.version.cuda if torch.cuda.is_available() else 'N/A'\n",
    "    },\n",
    "    'training': {\n",
    "        'total_episodes': len(episode_rewards),\n",
    "        'total_steps': agent.steps,\n",
    "        'best_score': best_score,\n",
    "        'best_reward': best_reward,\n",
    "        'final_epsilon': agent.epsilon\n",
    "    },\n",
    "    'test_results': {\n",
    "        'num_episodes': num_test_episodes,\n",
    "        'avg_score': float(np.mean(test_scores)),\n",
    "        'avg_reward': float(np.mean(test_rewards)),\n",
    "        'best_test_score': float(max(test_scores))\n",
    "    },\n",
    "    'config': CONFIG\n",
    "}\n",
    "\n",
    "with open(f'logs/training_summary_{timestamp}.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Training summary saved to logs/training_summary_{timestamp}.json\")\n",
    "print(\"\\nüéâ Training pipeline complete!\")\n",
    "print(f\"üöÄ GPU-accelerated training using {summary['hardware']['gpu']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DINO_TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
