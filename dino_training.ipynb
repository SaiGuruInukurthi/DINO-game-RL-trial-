{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78f9a3c2",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f58054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
    "from browser_dino_env import BrowserDinoEnv\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"Num GPUs: {len(tf.config.list_physical_devices('GPU'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadcbec6",
   "metadata": {},
   "source": [
    "## 2. GPU Configuration for RTX 3050 Mobile (4GB VRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c38b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure GPU memory growth to prevent VRAM overflow\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Enable memory growth - allocate only as needed\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        # Optional: Set memory limit if needed (e.g., 3.5GB out of 4GB)\n",
    "        # tf.config.set_logical_device_configuration(\n",
    "        #     gpus[0],\n",
    "        #     [tf.config.LogicalDeviceConfiguration(memory_limit=3584)]\n",
    "        # )\n",
    "        \n",
    "        print(\"‚úì GPU memory growth enabled\")\n",
    "        print(f\"‚úì Using GPU: {gpus[0].name}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration error: {e}\")\n",
    "else:\n",
    "    print(\"‚ö† No GPU detected - training will use CPU (slower)\")\n",
    "\n",
    "# Set mixed precision for better performance on RTX 3050\n",
    "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.set_global_policy(policy)\n",
    "print(f\"‚úì Mixed precision policy: {policy.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2f46dc",
   "metadata": {},
   "source": [
    "## 3. DQN Agent with CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c82b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    Deep Q-Network agent with CNN for processing game screenshots.\n",
    "    Optimized for 4GB VRAM with efficient memory usage.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_shape=(80, 80, 1),  # Grayscale 80x80 images\n",
    "        action_size=3,             # run, jump, duck\n",
    "        learning_rate=0.00025,\n",
    "        gamma=0.99,                # Discount factor\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.1,\n",
    "        epsilon_decay_steps=10000,\n",
    "        batch_size=32,             # Small batch for 4GB VRAM\n",
    "        memory_size=10000,         # Replay buffer size\n",
    "        target_update_freq=1000    # Update target network every N steps\n",
    "    ):\n",
    "        self.state_shape = state_shape\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Exploration parameters\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps\n",
    "        self.epsilon_decay_rate = (epsilon_start - epsilon_end) / epsilon_decay_steps\n",
    "        \n",
    "        # Training parameters\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.steps = 0\n",
    "        \n",
    "        # Experience replay buffer\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        \n",
    "        # Create Q-network and target network\n",
    "        self.q_network = self._build_model()\n",
    "        self.target_network = self._build_model()\n",
    "        self.update_target_network()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        \n",
    "        print(\"‚úì DQN Agent initialized\")\n",
    "        print(f\"  - State shape: {state_shape}\")\n",
    "        print(f\"  - Action size: {action_size}\")\n",
    "        print(f\"  - Batch size: {batch_size}\")\n",
    "        print(f\"  - Memory size: {memory_size}\")\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Build CNN architecture for processing game screenshots.\n",
    "        \n",
    "        Architecture:\n",
    "        - Conv2D(32, 8x8, stride 4) ‚Üí ReLU\n",
    "        - Conv2D(64, 4x4, stride 2) ‚Üí ReLU\n",
    "        - Conv2D(64, 3x3, stride 1) ‚Üí ReLU\n",
    "        - Flatten\n",
    "        - Dense(512) ‚Üí ReLU\n",
    "        - Dense(action_size) ‚Üí Linear (Q-values)\n",
    "        \"\"\"\n",
    "        inputs = layers.Input(shape=self.state_shape)\n",
    "        \n",
    "        # Normalize pixel values to [0, 1]\n",
    "        x = layers.Lambda(lambda x: x / 255.0)(inputs)\n",
    "        \n",
    "        # Convolutional layers\n",
    "        x = layers.Conv2D(32, (8, 8), strides=4, activation='relu', \n",
    "                         kernel_initializer='he_normal')(x)\n",
    "        x = layers.Conv2D(64, (4, 4), strides=2, activation='relu',\n",
    "                         kernel_initializer='he_normal')(x)\n",
    "        x = layers.Conv2D(64, (3, 3), strides=1, activation='relu',\n",
    "                         kernel_initializer='he_normal')(x)\n",
    "        \n",
    "        # Flatten and dense layers\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(512, activation='relu',\n",
    "                        kernel_initializer='he_normal')(x)\n",
    "        \n",
    "        # Output layer - Q-values for each action\n",
    "        outputs = layers.Dense(self.action_size, activation='linear',\n",
    "                              kernel_initializer='he_normal',\n",
    "                              dtype='float32')(x)  # Force float32 for stability\n",
    "        \n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        return model\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy weights from Q-network to target network\"\"\"\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay buffer\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state, training=True):\n",
    "        \"\"\"\n",
    "        Choose action using epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current game state (80x80 grayscale image)\n",
    "            training: If True, use epsilon-greedy; if False, use greedy\n",
    "        \n",
    "        Returns:\n",
    "            action: Integer action (0=run, 1=jump, 2=duck)\n",
    "        \"\"\"\n",
    "        # Exploration: random action\n",
    "        if training and np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        \n",
    "        # Exploitation: best action from Q-network\n",
    "        state_batch = np.expand_dims(state, axis=0)  # Add batch dimension\n",
    "        q_values = self.q_network.predict(state_batch, verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"\n",
    "        Train on a batch of experiences from replay buffer.\n",
    "        \n",
    "        Returns:\n",
    "            loss: Training loss for monitoring\n",
    "        \"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0.0\n",
    "        \n",
    "        # Sample random batch\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([exp[0] for exp in batch])\n",
    "        actions = np.array([exp[1] for exp in batch])\n",
    "        rewards = np.array([exp[2] for exp in batch])\n",
    "        next_states = np.array([exp[3] for exp in batch])\n",
    "        dones = np.array([exp[4] for exp in batch])\n",
    "        \n",
    "        # Compute target Q-values using target network\n",
    "        next_q_values = self.target_network.predict(next_states, verbose=0)\n",
    "        max_next_q = np.max(next_q_values, axis=1)\n",
    "        \n",
    "        # Q-learning target: r + Œ≥ * max(Q(s', a')) if not done, else r\n",
    "        targets = rewards + (1 - dones) * self.gamma * max_next_q\n",
    "        \n",
    "        # Train Q-network\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Get current Q-values\n",
    "            q_values = self.q_network(states, training=True)\n",
    "            \n",
    "            # Select Q-values for actions taken\n",
    "            action_masks = tf.one_hot(actions, self.action_size)\n",
    "            q_action = tf.reduce_sum(q_values * action_masks, axis=1)\n",
    "            \n",
    "            # Compute loss (MSE between predicted and target Q-values)\n",
    "            loss = tf.reduce_mean(tf.square(targets - q_action))\n",
    "        \n",
    "        # Backpropagation\n",
    "        gradients = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.q_network.trainable_variables))\n",
    "        \n",
    "        return loss.numpy()\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        \"\"\"Decay epsilon for exploration\"\"\"\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon -= self.epsilon_decay_rate\n",
    "            self.epsilon = max(self.epsilon_end, self.epsilon)\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save model weights\"\"\"\n",
    "        self.q_network.save_weights(filepath)\n",
    "        print(f\"‚úì Model saved to {filepath}\")\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        \"\"\"Load model weights\"\"\"\n",
    "        self.q_network.load_weights(filepath)\n",
    "        self.update_target_network()\n",
    "        print(f\"‚úì Model loaded from {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e282be31",
   "metadata": {},
   "source": [
    "## 4. Create Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfa2338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy agent to visualize architecture\n",
    "dummy_agent = DQNAgent()\n",
    "dummy_agent.q_network.summary()\n",
    "\n",
    "# Calculate approximate VRAM usage\n",
    "total_params = dummy_agent.q_network.count_params()\n",
    "param_size_mb = (total_params * 4) / (1024 ** 2)  # 4 bytes per float32\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Approximate size: {param_size_mb:.2f} MB\")\n",
    "print(f\"  Estimated VRAM usage (with batch): ~{param_size_mb * 3:.2f} MB\")\n",
    "print(f\"  Safe for RTX 3050 4GB: {'‚úì Yes' if param_size_mb * 3 < 3000 else '‚úó No'}\")\n",
    "\n",
    "del dummy_agent  # Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eec64c5",
   "metadata": {},
   "source": [
    "## 5. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56897a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "CONFIG = {\n",
    "    # Training\n",
    "    'num_episodes': 1000,\n",
    "    'max_steps_per_episode': 5000,\n",
    "    \n",
    "    # DQN parameters\n",
    "    'learning_rate': 0.00025,\n",
    "    'gamma': 0.99,\n",
    "    'batch_size': 32,\n",
    "    'memory_size': 10000,\n",
    "    'target_update_freq': 1000,\n",
    "    \n",
    "    # Exploration\n",
    "    'epsilon_start': 1.0,\n",
    "    'epsilon_end': 0.1,\n",
    "    'epsilon_decay_steps': 10000,\n",
    "    \n",
    "    # Logging\n",
    "    'log_interval': 10,        # Print stats every N episodes\n",
    "    'save_interval': 50,       # Save model every N episodes\n",
    "    'plot_interval': 50,       # Update plots every N episodes\n",
    "}\n",
    "\n",
    "# Create directories for saving\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "print(\"üìã Training Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf81264",
   "metadata": {},
   "source": [
    "## 6. Initialize Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "print(\"üéÆ Initializing Chrome Dino environment...\")\n",
    "env = BrowserDinoEnv()\n",
    "print(f\"‚úì Environment initialized\")\n",
    "print(f\"  Observation space: {env.observation_space}\")\n",
    "print(f\"  Action space: {env.action_space}\")\n",
    "\n",
    "# Initialize agent\n",
    "print(\"\\nü§ñ Initializing DQN agent...\")\n",
    "agent = DQNAgent(\n",
    "    state_shape=(80, 80, 1),\n",
    "    action_size=3,\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    gamma=CONFIG['gamma'],\n",
    "    epsilon_start=CONFIG['epsilon_start'],\n",
    "    epsilon_end=CONFIG['epsilon_end'],\n",
    "    epsilon_decay_steps=CONFIG['epsilon_decay_steps'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    memory_size=CONFIG['memory_size'],\n",
    "    target_update_freq=CONFIG['target_update_freq']\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Ready to train!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684f3e3c",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8a097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "episode_rewards = []\n",
    "episode_scores = []\n",
    "episode_losses = []\n",
    "episode_lengths = []\n",
    "epsilon_history = []\n",
    "\n",
    "# Best performance tracking\n",
    "best_score = 0\n",
    "best_reward = float('-inf')\n",
    "\n",
    "print(\"üöÄ Starting training...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    for episode in range(CONFIG['num_episodes']):\n",
    "        # Reset environment\n",
    "        state, info = env.reset()\n",
    "        state = np.expand_dims(state, axis=-1)  # Add channel dimension\n",
    "        \n",
    "        episode_reward = 0\n",
    "        episode_loss = []\n",
    "        done = False\n",
    "        step = 0\n",
    "        \n",
    "        # Episode loop\n",
    "        while not done and step < CONFIG['max_steps_per_episode']:\n",
    "            # Select action\n",
    "            action = agent.act(state, training=True)\n",
    "            \n",
    "            # Take step in environment\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_state = np.expand_dims(next_state, axis=-1)\n",
    "            \n",
    "            # Store experience\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Train agent\n",
    "            loss = agent.replay()\n",
    "            if loss > 0:\n",
    "                episode_loss.append(loss)\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            step += 1\n",
    "            agent.steps += 1\n",
    "            \n",
    "            # Update target network\n",
    "            if agent.steps % agent.target_update_freq == 0:\n",
    "                agent.update_target_network()\n",
    "            \n",
    "            # Decay epsilon\n",
    "            agent.update_epsilon()\n",
    "        \n",
    "        # Episode complete - record metrics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_scores.append(info.get('score', 0))\n",
    "        episode_lengths.append(step)\n",
    "        episode_losses.append(np.mean(episode_loss) if episode_loss else 0)\n",
    "        epsilon_history.append(agent.epsilon)\n",
    "        \n",
    "        # Update best performance\n",
    "        if info.get('score', 0) > best_score:\n",
    "            best_score = info.get('score', 0)\n",
    "            agent.save(f'models/best_model.weights.h5')\n",
    "        \n",
    "        if episode_reward > best_reward:\n",
    "            best_reward = episode_reward\n",
    "        \n",
    "        # Logging\n",
    "        if (episode + 1) % CONFIG['log_interval'] == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-CONFIG['log_interval']:])\n",
    "            avg_score = np.mean(episode_scores[-CONFIG['log_interval']:])\n",
    "            avg_loss = np.mean(episode_losses[-CONFIG['log_interval']:])\n",
    "            avg_length = np.mean(episode_lengths[-CONFIG['log_interval']:])\n",
    "            \n",
    "            print(f\"Episode {episode + 1}/{CONFIG['num_episodes']}\")\n",
    "            print(f\"  Avg Reward: {avg_reward:.2f} | Avg Score: {avg_score:.0f} | \"\n",
    "                  f\"Avg Steps: {avg_length:.0f}\")\n",
    "            print(f\"  Loss: {avg_loss:.4f} | Epsilon: {agent.epsilon:.3f} | \"\n",
    "                  f\"Memory: {len(agent.memory)}/{CONFIG['memory_size']}\")\n",
    "            print(f\"  Best Score: {best_score:.0f} | Best Reward: {best_reward:.2f}\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (episode + 1) % CONFIG['save_interval'] == 0:\n",
    "            agent.save(f'models/checkpoint_ep{episode + 1}.weights.h5')\n",
    "            \n",
    "            # Save metrics\n",
    "            metrics = {\n",
    "                'episode': episode + 1,\n",
    "                'episode_rewards': episode_rewards,\n",
    "                'episode_scores': episode_scores,\n",
    "                'episode_losses': episode_losses,\n",
    "                'episode_lengths': episode_lengths,\n",
    "                'epsilon_history': epsilon_history,\n",
    "                'best_score': best_score,\n",
    "                'best_reward': best_reward,\n",
    "                'config': CONFIG\n",
    "            }\n",
    "            \n",
    "            with open(f'logs/metrics_ep{episode + 1}.json', 'w') as f:\n",
    "                json.dump(metrics, f, indent=2)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö† Training interrupted by user\")\n",
    "    print(\"Saving current progress...\")\n",
    "    agent.save(f'models/interrupted_ep{episode}.weights.h5')\n",
    "\n",
    "finally:\n",
    "    print(\"\\nüèÅ Training complete!\")\n",
    "    print(f\"  Total episodes: {len(episode_rewards)}\")\n",
    "    print(f\"  Best score: {best_score}\")\n",
    "    print(f\"  Best reward: {best_reward:.2f}\")\n",
    "    \n",
    "    # Close environment\n",
    "    env.close()\n",
    "    print(\"‚úì Browser closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a49e4",
   "metadata": {},
   "source": [
    "## 8. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55176299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('DQN Training Metrics', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Rewards\n",
    "axes[0, 0].plot(episode_rewards, alpha=0.3, label='Episode Reward')\n",
    "axes[0, 0].plot(np.convolve(episode_rewards, np.ones(50)/50, mode='valid'), \n",
    "                label='Moving Avg (50)', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Total Reward')\n",
    "axes[0, 0].set_title('Episode Rewards')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scores\n",
    "axes[0, 1].plot(episode_scores, alpha=0.3, label='Episode Score')\n",
    "axes[0, 1].plot(np.convolve(episode_scores, np.ones(50)/50, mode='valid'),\n",
    "                label='Moving Avg (50)', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Score (Distance)')\n",
    "axes[0, 1].set_title('Game Scores')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1, 0].plot(episode_losses, alpha=0.6)\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].set_title('Training Loss')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Epsilon\n",
    "axes[1, 1].plot(epsilon_history, color='orange')\n",
    "axes[1, 1].set_xlabel('Episode')\n",
    "axes[1, 1].set_ylabel('Epsilon')\n",
    "axes[1, 1].set_title('Exploration Rate (Epsilon)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/training_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Training plots saved to plots/training_metrics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5bf118",
   "metadata": {},
   "source": [
    "## 9. Test Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d9889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the best model\n",
    "print(\"üéÆ Testing trained agent...\\n\")\n",
    "\n",
    "# Load best model\n",
    "agent.load('models/best_model.weights.h5')\n",
    "agent.epsilon = 0.0  # No exploration during testing\n",
    "\n",
    "# Run test episodes\n",
    "num_test_episodes = 5\n",
    "test_scores = []\n",
    "test_rewards = []\n",
    "\n",
    "env = BrowserDinoEnv()\n",
    "\n",
    "for episode in range(num_test_episodes):\n",
    "    state, info = env.reset()\n",
    "    state = np.expand_dims(state, axis=-1)\n",
    "    \n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "    \n",
    "    while not done and step < 5000:\n",
    "        action = agent.act(state, training=False)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        state = np.expand_dims(next_state, axis=-1)\n",
    "        episode_reward += reward\n",
    "        step += 1\n",
    "    \n",
    "    test_scores.append(info.get('score', 0))\n",
    "    test_rewards.append(episode_reward)\n",
    "    \n",
    "    print(f\"Test Episode {episode + 1}: Score={info.get('score', 0):.0f}, \"\n",
    "          f\"Reward={episode_reward:.2f}, Steps={step}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\nüìä Test Results:\")\n",
    "print(f\"  Average Score: {np.mean(test_scores):.0f} ¬± {np.std(test_scores):.0f}\")\n",
    "print(f\"  Average Reward: {np.mean(test_rewards):.2f} ¬± {np.std(test_rewards):.2f}\")\n",
    "print(f\"  Best Score: {max(test_scores):.0f}\")\n",
    "print(f\"  Worst Score: {min(test_scores):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9841c5b2",
   "metadata": {},
   "source": [
    "## 10. Save Final Model and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cfb066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final summary\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "summary = {\n",
    "    'timestamp': timestamp,\n",
    "    'hardware': {\n",
    "        'gpu': 'RTX 3050 Mobile',\n",
    "        'vram': '4GB',\n",
    "        'tensorflow_version': tf.__version__\n",
    "    },\n",
    "    'training': {\n",
    "        'total_episodes': len(episode_rewards),\n",
    "        'total_steps': agent.steps,\n",
    "        'best_score': best_score,\n",
    "        'best_reward': best_reward,\n",
    "        'final_epsilon': agent.epsilon\n",
    "    },\n",
    "    'test_results': {\n",
    "        'num_episodes': num_test_episodes,\n",
    "        'avg_score': float(np.mean(test_scores)),\n",
    "        'avg_reward': float(np.mean(test_rewards)),\n",
    "        'best_test_score': float(max(test_scores))\n",
    "    },\n",
    "    'config': CONFIG\n",
    "}\n",
    "\n",
    "with open(f'logs/training_summary_{timestamp}.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Training summary saved to logs/training_summary_{timestamp}.json\")\n",
    "print(\"\\nüéâ Training pipeline complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
