{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78f9a3c2",
   "metadata": {},
   "source": [
    "# Chrome Dino RL Training - DQN with CNN (PyTorch + GPU)\n",
    "\n",
    "**Hardware Configuration:**\n",
    "- GPU: RTX 3050 Mobile (4GB VRAM) ‚úÖ CUDA Enabled\n",
    "- Drivers: NVIDIA Studio Drivers\n",
    "- Framework: PyTorch 2.5.1 with CUDA 12.1\n",
    "\n",
    "**Model Architecture:**\n",
    "- Deep Q-Network (DQN) with Convolutional Neural Networks\n",
    "- Input: 80x80 grayscale game screenshots\n",
    "- Output: Q-values for 3 actions (run, jump, duck)\n",
    "\n",
    "**Training Strategy:**\n",
    "- Experience Replay Buffer (10,000 transitions)\n",
    "- Target Network (updated every 1,000 steps)\n",
    "- Epsilon-greedy exploration (1.0 ‚Üí 0.1 over 10,000 steps)\n",
    "- Batch size: 32 (optimized for 4GB VRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a4f58054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "GPU device: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "CUDA version: 12.1\n",
      "GPU memory: 4.00 GB\n",
      "\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
    "from browser_dino_env import BrowserDinoEnv\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö† No GPU detected - training will use CPU\")\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadcbec6",
   "metadata": {},
   "source": [
    "## 2. GPU Configuration for RTX 3050 Mobile (4GB VRAM)\n",
    "\n",
    "PyTorch with CUDA 12.1 provides native GPU acceleration on Windows.\n",
    "Your RTX 3050 Mobile GPU is ready for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2c38b400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "‚úì Total VRAM: 4.00 GB\n",
      "‚úì Allocated: 0.05 GB\n",
      "‚úì Cached: 0.07 GB\n",
      "‚úì cuDNN benchmark: True\n"
     ]
    }
   ],
   "source": [
    "# GPU memory management\n",
    "if torch.cuda.is_available():\n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Set memory fraction (use ~90% of VRAM, leave some for Chrome)\n",
    "    torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "    \n",
    "    # Enable cuDNN benchmarking for optimal performance\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # Print memory info\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"‚úì GPU: {props.name}\")\n",
    "    print(f\"‚úì Total VRAM: {props.total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"‚úì Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"‚úì Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"‚úì cuDNN benchmark: {torch.backends.cudnn.benchmark}\")\n",
    "else:\n",
    "    print(\"‚ö† No GPU - training will be slow on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2f46dc",
   "metadata": {},
   "source": [
    "## 3. DQN Network with CNN Architecture (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "80c82b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q-Network with CNN for processing game screenshots.\n",
    "    \n",
    "    Architecture:\n",
    "    - Conv2D(32, 8x8, stride 4) ‚Üí ReLU\n",
    "    - Conv2D(64, 4x4, stride 2) ‚Üí ReLU\n",
    "    - Conv2D(64, 3x3, stride 1) ‚Üí ReLU\n",
    "    - Flatten\n",
    "    - Dense(512) ‚Üí ReLU\n",
    "    - Dense(action_size) ‚Üí Linear (Q-values)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape=(1, 80, 80), action_size=3):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # Calculate flattened size\n",
    "        def conv2d_size_out(size, kernel_size, stride):\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(80, 8, 4), 4, 2), 3, 1)\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(80, 8, 4), 4, 2), 3, 1)\n",
    "        linear_input_size = convw * convh * 64\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(linear_input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Normalize to [0, 1]\n",
    "        x = x.float() / 255.0\n",
    "        \n",
    "        # Conv layers with ReLU\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # FC layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    DQN Agent with experience replay and target network.\n",
    "    Optimized for 4GB VRAM with efficient memory usage.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_shape=(1, 80, 80),  # (channels, height, width)\n",
    "        action_size=3,             # run, jump, duck\n",
    "        learning_rate=0.00025,\n",
    "        gamma=0.99,                # Discount factor\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.1,\n",
    "        epsilon_decay_steps=10000,\n",
    "        batch_size=32,\n",
    "        memory_size=10000,\n",
    "        target_update_freq=1000\n",
    "    ):\n",
    "        self.state_shape = state_shape\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        \n",
    "        # Exploration parameters\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps\n",
    "        self.epsilon_decay_rate = (epsilon_start - epsilon_end) / epsilon_decay_steps\n",
    "        \n",
    "        # Training parameters\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.steps = 0\n",
    "        \n",
    "        # Experience replay buffer\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        \n",
    "        # Create Q-network and target network\n",
    "        self.q_network = DQN(state_shape, action_size).to(self.device)\n",
    "        self.target_network = DQN(state_shape, action_size).to(self.device)\n",
    "        self.update_target_network()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "        print(\"‚úì DQN Agent initialized\")\n",
    "        print(f\"  - State shape: {state_shape}\")\n",
    "        print(f\"  - Action size: {action_size}\")\n",
    "        print(f\"  - Batch size: {batch_size}\")\n",
    "        print(f\"  - Memory size: {memory_size}\")\n",
    "        print(f\"  - Device: {self.device}\")\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy weights from Q-network to target network\"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay buffer\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state, training=True):\n",
    "        \"\"\"\n",
    "        Choose action using epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current game state (80x80 grayscale image)\n",
    "            training: If True, use epsilon-greedy; if False, use greedy\n",
    "        \n",
    "        Returns:\n",
    "            action: Integer action (0=run, 1=jump, 2=duck)\n",
    "        \"\"\"\n",
    "        # Exploration: random action\n",
    "        if training and np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        \n",
    "        # Exploitation: best action from Q-network\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.q_network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"\n",
    "        Train on a batch of experiences from replay buffer.\n",
    "        \n",
    "        Returns:\n",
    "            loss: Training loss for monitoring\n",
    "        \"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0.0\n",
    "        \n",
    "        # Sample random batch\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states = torch.FloatTensor(np.array([exp[0] for exp in batch])).to(self.device)\n",
    "        actions = torch.LongTensor([exp[1] for exp in batch]).to(self.device)\n",
    "        rewards = torch.FloatTensor([exp[2] for exp in batch]).to(self.device)\n",
    "        next_states = torch.FloatTensor(np.array([exp[3] for exp in batch])).to(self.device)\n",
    "        dones = torch.FloatTensor([exp[4] for exp in batch]).to(self.device)\n",
    "        \n",
    "        # Get current Q-values\n",
    "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_network(next_states).max(1)[0]\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "        \n",
    "        # Compute loss and backpropagate\n",
    "        loss = self.loss_fn(current_q, target_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 10.0)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        \"\"\"Decay epsilon for exploration\"\"\"\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon -= self.epsilon_decay_rate\n",
    "            self.epsilon = max(self.epsilon_end, self.epsilon)\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save model weights\"\"\"\n",
    "        torch.save({\n",
    "            'q_network': self.q_network.state_dict(),\n",
    "            'target_network': self.target_network.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'steps': self.steps,\n",
    "            'epsilon': self.epsilon\n",
    "        }, filepath)\n",
    "        print(f\"‚úì Model saved to {filepath}\")\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        \"\"\"Load model weights\"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.q_network.load_state_dict(checkpoint['q_network'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_network'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.steps = checkpoint.get('steps', 0)\n",
    "        self.epsilon = checkpoint.get('epsilon', self.epsilon_end)\n",
    "        print(f\"‚úì Model loaded from {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e282be31",
   "metadata": {},
   "source": [
    "## 4. Create Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1dfa2338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì DQN Agent initialized\n",
      "  - State shape: (1, 80, 80)\n",
      "  - Action size: 3\n",
      "  - Batch size: 32\n",
      "  - Memory size: 10000\n",
      "  - Device: cuda\n",
      "\n",
      "üìä Network Architecture:\n",
      "DQN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=2304, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=3, bias=True)\n",
      ")\n",
      "\n",
      "üìä Model Statistics:\n",
      "  Total parameters: 1,253,539\n",
      "  Trainable parameters: 1,253,539\n",
      "  Approximate size: 4.78 MB\n",
      "  Estimated VRAM usage (with batch): ~14.35 MB\n",
      "  Safe for RTX 3050 4GB: ‚úì Yes\n",
      "\n",
      "üíæ GPU Memory after forward pass:\n",
      "  Allocated: 53.60 MB\n",
      "  Reserved: 90.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy agent to visualize architecture\n",
    "dummy_agent = DQNAgent()\n",
    "print(\"\\nüìä Network Architecture:\")\n",
    "print(dummy_agent.q_network)\n",
    "\n",
    "# Calculate parameters\n",
    "total_params = sum(p.numel() for p in dummy_agent.q_network.parameters())\n",
    "trainable_params = sum(p.numel() for p in dummy_agent.q_network.parameters() if p.requires_grad)\n",
    "param_size_mb = (total_params * 4) / (1024 ** 2)  # 4 bytes per float32\n",
    "\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Approximate size: {param_size_mb:.2f} MB\")\n",
    "print(f\"  Estimated VRAM usage (with batch): ~{param_size_mb * 3:.2f} MB\")\n",
    "print(f\"  Safe for RTX 3050 4GB: {'‚úì Yes' if param_size_mb * 3 < 3000 else '‚úó No'}\")\n",
    "\n",
    "# Check GPU memory if available\n",
    "if torch.cuda.is_available():\n",
    "    dummy_input = torch.randn(32, 1, 80, 80).to(device)\n",
    "    _ = dummy_agent.q_network(dummy_input)\n",
    "    print(f\"\\nüíæ GPU Memory after forward pass:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "del dummy_agent  # Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eec64c5",
   "metadata": {},
   "source": [
    "## 5. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "56897a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Training Configuration:\n",
      "  num_episodes: 1000\n",
      "  max_steps_per_episode: 5000\n",
      "  learning_rate: 0.00025\n",
      "  gamma: 0.99\n",
      "  batch_size: 32\n",
      "  memory_size: 10000\n",
      "  target_update_freq: 1000\n",
      "  epsilon_start: 1.0\n",
      "  epsilon_end: 0.1\n",
      "  epsilon_decay_steps: 10000\n",
      "  log_interval: 10\n",
      "  save_interval: 50\n",
      "  plot_interval: 50\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "CONFIG = {\n",
    "    # Training\n",
    "    'num_episodes': 1000,\n",
    "    'max_steps_per_episode': 5000,\n",
    "    \n",
    "    # DQN parameters\n",
    "    'learning_rate': 0.00025,\n",
    "    'gamma': 0.99,\n",
    "    'batch_size': 32,\n",
    "    'memory_size': 10000,\n",
    "    'target_update_freq': 1000,\n",
    "    \n",
    "    # Exploration\n",
    "    'epsilon_start': 1.0,\n",
    "    'epsilon_end': 0.1,\n",
    "    'epsilon_decay_steps': 10000,\n",
    "    \n",
    "    # Logging\n",
    "    'log_interval': 10,        # Print stats every N episodes\n",
    "    'save_interval': 50,       # Save model every N episodes\n",
    "    'plot_interval': 50,       # Update plots every N episodes\n",
    "}\n",
    "\n",
    "# Create directories for saving\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "print(\"üìã Training Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf81264",
   "metadata": {},
   "source": [
    "## 7. Initialize Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2c60bd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelDinoEnvs:\n",
    "    \"\"\"\n",
    "    Wrapper for running multiple Chrome Dino environments in parallel.\n",
    "    Each environment runs in its own browser window.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_envs=4):\n",
    "        self.num_envs = num_envs\n",
    "        self.envs = []\n",
    "        \n",
    "        print(f\"üöÄ Initializing {num_envs} parallel environments...\")\n",
    "        for i in range(num_envs):\n",
    "            try:\n",
    "                env = BrowserDinoEnv()\n",
    "                self.envs.append(env)\n",
    "                print(f\"  ‚úì Environment {i+1}/{num_envs} ready\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó Environment {i+1} failed: {e}\")\n",
    "                # Clean up on failure\n",
    "                for env in self.envs:\n",
    "                    env.close()\n",
    "                raise\n",
    "        \n",
    "        print(f\"‚úÖ All {num_envs} environments initialized!\")\n",
    "        \n",
    "        # Store observation/action space from first env\n",
    "        self.observation_space = self.envs[0].observation_space\n",
    "        self.action_space = self.envs[0].action_space\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset all environments and return initial states\"\"\"\n",
    "        states = []\n",
    "        infos = []\n",
    "        \n",
    "        for env in self.envs:\n",
    "            state, info = env.reset()\n",
    "            states.append(state)\n",
    "            infos.append(info)\n",
    "        \n",
    "        return np.array(states), infos\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Take actions in all environments.\n",
    "        \n",
    "        Args:\n",
    "            actions: List of actions (one per environment)\n",
    "        \n",
    "        Returns:\n",
    "            states: Array of next states\n",
    "            rewards: Array of rewards\n",
    "            terminateds: Array of terminated flags\n",
    "            truncateds: Array of truncated flags\n",
    "            infos: List of info dicts\n",
    "        \"\"\"\n",
    "        states = []\n",
    "        rewards = []\n",
    "        terminateds = []\n",
    "        truncateds = []\n",
    "        infos = []\n",
    "        \n",
    "        for env, action in zip(self.envs, actions):\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            states.append(state)\n",
    "            rewards.append(reward)\n",
    "            terminateds.append(terminated)\n",
    "            truncateds.append(truncated)\n",
    "            infos.append(info)\n",
    "        \n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(rewards),\n",
    "            np.array(terminateds),\n",
    "            np.array(truncateds),\n",
    "            infos\n",
    "        )\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close all environments\"\"\"\n",
    "        print(f\"\\nüîí Closing {self.num_envs} environments...\")\n",
    "        for i, env in enumerate(self.envs):\n",
    "            try:\n",
    "                env.close()\n",
    "                print(f\"  ‚úì Environment {i+1} closed\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó Environment {i+1} error: {e}\")\n",
    "        print(\"‚úì All environments closed\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_envs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300857b4",
   "metadata": {},
   "source": [
    "## 6. Parallel Environment Wrapper\n",
    "\n",
    "Training on 4 browser windows simultaneously will:\n",
    "- 4x faster experience collection\n",
    "- Better sample diversity\n",
    "- More stable training\n",
    "- ~2-4 hours for 1000 episodes (vs 8-15 hours single env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e80f089e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéÆ Initializing Chrome Dino environments...\n",
      "‚úì Ads and unnecessary content hidden\n",
      "‚úì Environment(s) initialized\n",
      "  Observation space: Box(0, 255, (80, 80), uint8)\n",
      "  Action space: Discrete(2)\n",
      "  Parallel envs: 1\n",
      "\n",
      "ü§ñ Initializing DQN agent...\n",
      "‚úì DQN Agent initialized\n",
      "  - State shape: (1, 80, 80)\n",
      "  - Action size: 3\n",
      "  - Batch size: 32\n",
      "  - Memory size: 10000\n",
      "  - Device: cuda\n",
      "\n",
      "‚úÖ Ready to train on GPU with 1x parallel collection!\n"
     ]
    }
   ],
   "source": [
    "# Choose number of parallel environments (1-4 recommended for 4GB VRAM)\n",
    "NUM_PARALLEL_ENVS = 1\n",
    "\n",
    "# Initialize parallel environments\n",
    "print(\"üéÆ Initializing Chrome Dino environments...\")\n",
    "if NUM_PARALLEL_ENVS > 1:\n",
    "    env = ParallelDinoEnvs(num_envs=NUM_PARALLEL_ENVS)\n",
    "else:\n",
    "    env = BrowserDinoEnv()\n",
    "    \n",
    "print(f\"‚úì Environment(s) initialized\")\n",
    "print(f\"  Observation space: {env.observation_space}\")\n",
    "print(f\"  Action space: {env.action_space}\")\n",
    "print(f\"  Parallel envs: {NUM_PARALLEL_ENVS}\")\n",
    "\n",
    "# Initialize agent\n",
    "print(\"\\nü§ñ Initializing DQN agent...\")\n",
    "agent = DQNAgent(\n",
    "    state_shape=(1, 80, 80),  # PyTorch uses (C, H, W) format\n",
    "    action_size=3,\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    gamma=CONFIG['gamma'],\n",
    "    epsilon_start=CONFIG['epsilon_start'],\n",
    "    epsilon_end=CONFIG['epsilon_end'],\n",
    "    epsilon_decay_steps=CONFIG['epsilon_decay_steps'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    memory_size=CONFIG['memory_size'],\n",
    "    target_update_freq=CONFIG['target_update_freq']\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Ready to train on GPU with {NUM_PARALLEL_ENVS}x parallel collection!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b236bb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üÜï Starting fresh training with corrected score retrieval\n",
      "   (Score now matches displayed value - expect 30-50 range initially)\n",
      "\n",
      "‚úÖ Agent ready with epsilon=1.000, steps=0\n"
     ]
    }
   ],
   "source": [
    "# Resume training configuration\n",
    "# ‚ö†Ô∏è IMPORTANT: Score retrieval was fixed! Old checkpoints have inflated scores (40x too high)\n",
    "# Set RESUME_TRAINING = False to start fresh with corrected scores\n",
    "RESUME_TRAINING = False  # Set to True to resume from checkpoint (only if checkpoint uses correct scores)\n",
    "CHECKPOINT_PATH = 'models/best_model.pth'  # or 'models/checkpoint_ep50.pth'\n",
    "\n",
    "# Initialize metric variables\n",
    "episode_rewards = []\n",
    "episode_scores = []\n",
    "episode_losses = []\n",
    "episode_lengths = []\n",
    "epsilon_history = []\n",
    "best_score = 0\n",
    "best_reward = float('-inf')\n",
    "\n",
    "if RESUME_TRAINING and os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"üîÑ Resuming training from {CHECKPOINT_PATH}\")\n",
    "    agent.load(CHECKPOINT_PATH)\n",
    "    \n",
    "    # Try to load previous metrics to continue plotting\n",
    "    try:\n",
    "        # Find latest metrics file\n",
    "        metric_files = sorted([f for f in os.listdir('logs') if f.startswith('metrics_ep') and f.endswith('.json')])\n",
    "        if metric_files:\n",
    "            latest_metrics = metric_files[-1]\n",
    "            with open(f'logs/{latest_metrics}', 'r') as f:\n",
    "                prev_metrics = json.load(f)\n",
    "                \n",
    "                # Load previous metrics\n",
    "                episode_rewards = prev_metrics.get('episode_rewards', [])\n",
    "                episode_scores = prev_metrics.get('episode_scores', [])\n",
    "                episode_losses = prev_metrics.get('episode_losses', [])\n",
    "                episode_lengths = prev_metrics.get('episode_lengths', [])\n",
    "                epsilon_history = prev_metrics.get('epsilon_history', [])\n",
    "                best_score = prev_metrics.get('best_score', 0)\n",
    "                best_reward = prev_metrics.get('best_reward', float('-inf'))\n",
    "                \n",
    "                # Adjust episode count to continue from where we left off\n",
    "                completed_episodes = prev_metrics.get('episode', 0)\n",
    "                print(f\"‚úì Loaded metrics from episode {completed_episodes}\")\n",
    "                print(f\"  Previous best score: {best_score}\")\n",
    "                print(f\"  Previous best reward: {best_reward:.2f}\")\n",
    "                print(f\"  Current epsilon: {agent.epsilon:.3f}\")\n",
    "                print(f\"  Total steps: {agent.steps}\")\n",
    "                print(f\"  Memory buffer: {len(agent.memory)} experiences\")\n",
    "                \n",
    "                # Update CONFIG to train for additional episodes\n",
    "                print(f\"\\nüìã Training will continue for {CONFIG['num_episodes']} more episodes\")\n",
    "                print(f\"   (Total episodes will be: {completed_episodes + CONFIG['num_episodes']})\")\n",
    "        else:\n",
    "            print(\"‚ö† No previous metrics found, starting with loaded weights only\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Could not load previous metrics: {e}\")\n",
    "        print(\"  Will continue with loaded model weights only\")\n",
    "else:\n",
    "    if RESUME_TRAINING:\n",
    "        print(f\"‚ö† Checkpoint not found: {CHECKPOINT_PATH}\")\n",
    "        print(\"üÜï Starting fresh training instead\")\n",
    "    else:\n",
    "        print(\"üÜï Starting fresh training with corrected score retrieval\")\n",
    "        print(\"   (Score now matches displayed value - expect 30-50 range initially)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Agent ready with epsilon={agent.epsilon:.3f}, steps={agent.steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da32ef1f",
   "metadata": {},
   "source": [
    "## 7.5. Optional: Resume Training from Checkpoint\n",
    "\n",
    "Set `RESUME_TRAINING = True` to continue from a saved model instead of starting from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40706413",
   "metadata": {},
   "source": [
    "## 7.6. IMPORTANT: Restart Environments After Code Changes\n",
    "\n",
    "‚ö†Ô∏è **If you just updated the score retrieval in `browser_dino_env.py`, you MUST restart the environments!**\n",
    "\n",
    "The old browser instances are still running with the old code. Run the cell below to close and reinitialize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "53fa42aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Restarting environments to load updated code...\n",
      "‚úì Old environments closed\n",
      "‚úì Module reloaded with latest changes\n",
      "\n",
      "üéÆ Reinitializing 1 environments...\n",
      "‚úì Ads and unnecessary content hidden\n",
      "\n",
      "‚úÖ Environments restarted with corrected score retrieval!\n",
      "   Scores should now show realistic values (30-50 range initially)\n"
     ]
    }
   ],
   "source": [
    "# Close old environments and reinitialize with updated code\n",
    "print(\"üîÑ Restarting environments to load updated code...\")\n",
    "\n",
    "# Close existing environments\n",
    "try:\n",
    "    env.close()\n",
    "    print(\"‚úì Old environments closed\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Reimport the module to get latest code changes\n",
    "import importlib\n",
    "import sys\n",
    "if 'browser_dino_env' in sys.modules:\n",
    "    importlib.reload(sys.modules['browser_dino_env'])\n",
    "    from browser_dino_env import BrowserDinoEnv\n",
    "    print(\"‚úì Module reloaded with latest changes\")\n",
    "\n",
    "# Reinitialize environments\n",
    "print(f\"\\nüéÆ Reinitializing {NUM_PARALLEL_ENVS} environments...\")\n",
    "if NUM_PARALLEL_ENVS > 1:\n",
    "    env = ParallelDinoEnvs(num_envs=NUM_PARALLEL_ENVS)\n",
    "else:\n",
    "    env = BrowserDinoEnv()\n",
    "\n",
    "print(f\"\\n‚úÖ Environments restarted with corrected score retrieval!\")\n",
    "print(\"   Scores should now show realistic values (30-50 range initially)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684f3e3c",
   "metadata": {},
   "source": [
    "## 8. Training Loop (Parallel)\n",
    "\n",
    "Training will continue from loaded checkpoint if `RESUME_TRAINING = True` was set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4a8a097b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training on GPU with 1x parallel environments...\n",
      "üìù Progress will be saved to logs/training_progress.txt\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IC1807\\AppData\\Local\\Temp\\ipykernel_28228\\3717846263.py:153: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  dones = torch.FloatTensor([exp[4] for exp in batch]).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model saved to models/best_model.pth                                                                                  \n",
      "üèÜ New best score: 50\n",
      "Episode 1 complete | Score: 50 | Steps: 35 | Reward: 18.40\n",
      "Episode 2 complete | Score: 47 | Steps: 30 | Reward: 15.90                                                              \n",
      "Episode 3 complete | Score: 47 | Steps: 27 | Reward: 16.10                                                              \n",
      "Episode 4 complete | Score: 47 | Steps: 28 | Reward: 15.70                                                              \n",
      "Episode 5 complete | Score: 48 | Steps: 31 | Reward: 16.50                                                              \n",
      "Episode 6 complete | Score: 48 | Steps: 29 | Reward: 15.80                                                              \n",
      "‚úì Model saved to models/best_model.pth                                                                                  \n",
      "üèÜ New best score: 59\n",
      "Episode 7 complete | Score: 59 | Steps: 35 | Reward: 22.40\n",
      "Episode 8 complete | Score: 48 | Steps: 29 | Reward: 16.30                                                              \n",
      "Episode 9 complete | Score: 47 | Steps: 29 | Reward: 15.80                                                              \n",
      "                                                                                                                        \n",
      "Episode 10/1000 (1x parallel)\n",
      "  Avg Reward: 15.65 | Avg Score: 47 | Avg Steps: 29\n",
      "  Loss: 1.6982 | Epsilon: 0.974 | Memory: 290/10000 | GPU: 49MB\n",
      "  Best Score: 59 | Best Reward: 22.40\n",
      "  Improvement (last 10 eps): Reward +0.00 | Score +0.00\n",
      "--------------------------------------------------------------------------------\n",
      "Episode 11 complete | Score: 50 | Steps: 31 | Reward: 17.50                                                             \n",
      "Episode 12 complete | Score: 49 | Steps: 34 | Reward: 17.80                                                             \n",
      "Episode 13 complete | Score: 47 | Steps: 29 | Reward: 15.80                                                             \n",
      "‚úì Model saved to models/best_model.pth                                                                                  \n",
      "üèÜ New best score: 62\n",
      "Episode 14 complete | Score: 62 | Steps: 43 | Reward: 25.20\n",
      "Episode 15 complete | Score: 48 | Steps: 30 | Reward: 16.40 | Œî Reward: +1.01, Œî Score: +1.70                           \n",
      "Episode 16 complete | Score: 48 | Steps: 30 | Reward: 16.90                                                             \n",
      "Episode 17 complete | Score: 47 | Steps: 30 | Reward: 16.40                                                             \n",
      "Episode 18 complete | Score: 50 | Steps: 37 | Reward: 18.60                                                             \n",
      "Episode 19 complete | Score: 48 | Steps: 32 | Reward: 16.60                                                             \n",
      "                                                                                                                        \n",
      "Episode 20/1000 (1x parallel)\n",
      "  Avg Reward: 17.78 | Avg Score: 50 | Avg Steps: 33\n",
      "  Loss: 0.8096 | Epsilon: 0.944 | Memory: 618/10000 | GPU: 49MB\n",
      "  Best Score: 62 | Best Reward: 25.20\n",
      "  Improvement (last 10 eps): Reward +2.13 | Score +3.10\n",
      "--------------------------------------------------------------------------------\n",
      "Episode 21 complete | Score: 51 | Steps: 32 | Reward: 18.10                                                             \n",
      "‚úì Model saved to models/best_model.pth                                                                                  \n",
      "üèÜ New best score: 67\n",
      "Episode 22 complete | Score: 67 | Steps: 46 | Reward: 28.00\n",
      "Episode 23 complete | Score: 47 | Steps: 33 | Reward: 16.70                                                             \n",
      "Episode 24 complete | Score: 48 | Steps: 31 | Reward: 16.50                                                             \n",
      "Episode 25 complete | Score: 47 | Steps: 30 | Reward: 15.90 | Œî Reward: +1.37, Œî Score: +1.80                           \n",
      "Episode 26 complete | Score: 48 | Steps: 31 | Reward: 16.50                                                             \n",
      "Episode 27 complete | Score: 47 | Steps: 34 | Reward: 16.30                                                             \n",
      "Episode 28 complete | Score: 48 | Steps: 33 | Reward: 16.70                                                             \n",
      "‚úì Model saved to models/best_model.pth                                                                                  \n",
      "üèÜ New best score: 75\n",
      "Episode 29 complete | Score: 75 | Steps: 48 | Reward: 31.20\n",
      "                                                                                                                        \n",
      "Episode 30/1000 (1x parallel)\n",
      "  Avg Reward: 19.41 | Avg Score: 53 | Avg Steps: 35\n",
      "  Loss: 0.5103 | Epsilon: 0.913 | Memory: 969/10000 | GPU: 49MB\n",
      "  Best Score: 75 | Best Reward: 31.20\n",
      "  Improvement (last 10 eps): Reward +1.63 | Score +3.20\n",
      "--------------------------------------------------------------------------------\n",
      "Episode 31 complete | Score: 50 | Steps: 35 | Reward: 17.90                                                             \n",
      "Episode 32 complete | Score: 50 | Steps: 34 | Reward: 17.80                                                             \n",
      "Episode 33 complete | Score: 48 | Steps: 32 | Reward: 16.10                                                             \n",
      "Episode 34 complete | Score: 67 | Steps: 43 | Reward: 27.20                                                             \n",
      "Episode 35 complete | Score: 48 | Steps: 32 | Reward: 16.60 | Œî Reward: +1.42, Œî Score: +3.10                           \n",
      "Episode 36 complete | Score: 48 | Steps: 32 | Reward: 17.10                                                             \n",
      "Episode 37 complete | Score: 48 | Steps: 33 | Reward: 17.20                                                             \n",
      "Episode 38 complete | Score: 51 | Steps: 35 | Reward: 18.40                                                             \n",
      "Episode 39 complete | Score: 48 | Steps: 31 | Reward: 17.00                                                             \n",
      "                                                                                                                        \n",
      "Episode 40/1000 (1x parallel)\n",
      "  Avg Reward: 18.12 | Avg Score: 50 | Avg Steps: 34\n",
      "  Loss: 0.8663 | Epsilon: 0.882 | Memory: 1306/10000 | GPU: 49MB\n",
      "  Best Score: 75 | Best Reward: 31.20\n",
      "  Improvement (last 10 eps): Reward -1.29 | Score -2.40\n",
      "--------------------------------------------------------------------------------\n",
      "Episode 41 complete | Score: 51 | Steps: 32 | Reward: 18.60                                                             \n",
      "Episode 42 complete | Score: 47 | Steps: 33 | Reward: 16.70                                                             \n",
      "Episode 43 complete | Score: 48 | Steps: 30 | Reward: 16.40                                                             \n",
      "Episode 44 complete | Score: 53 | Steps: 33 | Reward: 18.70                                                             \n",
      "Episode 45 complete | Score: 55 | Steps: 37 | Reward: 20.60 | Œî Reward: -1.79, Œî Score: -3.60                           \n",
      "Episode 46 complete | Score: 48 | Steps: 31 | Reward: 16.50                                                             \n",
      "Episode 47 complete | Score: 48 | Steps: 31 | Reward: 16.50                                                             \n",
      "Episode 48 complete | Score: 51 | Steps: 33 | Reward: 18.20                                                             \n",
      "Episode 49 complete | Score: 50 | Steps: 35 | Reward: 18.40                                                             \n",
      "                                                                                                                        \n",
      "Episode 50/1000 (1x parallel)\n",
      "  Avg Reward: 17.72 | Avg Score: 50 | Avg Steps: 33\n",
      "  Loss: 0.5613 | Epsilon: 0.853 | Memory: 1633/10000 | GPU: 49MB\n",
      "  Best Score: 75 | Best Reward: 31.20\n",
      "  Improvement (last 10 eps): Reward -0.40 | Score -0.60\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì Model saved to models/checkpoint_ep50.pth\n",
      "üíæ Checkpoint saved: models/checkpoint_ep50.pth\n",
      "Episode 51 complete | Score: 48 | Steps: 31 | Reward: 16.00                                                             \n",
      "Episode 52 complete | Score: 48 | Steps: 34 | Reward: 17.30                                                             \n",
      "Episode 53 complete | Score: 51 | Steps: 34 | Reward: 17.80                                                             \n",
      "Episode 54 complete | Score: 50 | Steps: 33 | Reward: 17.70                                                             \n",
      "Episode 55 complete | Score: 51 | Steps: 35 | Reward: 18.40 | Œî Reward: -0.32, Œî Score: -0.30                           \n",
      "Episode 56 complete | Score: 47 | Steps: 32 | Reward: 16.10                                                             \n",
      "Episode 57 complete | Score: 48 | Steps: 29 | Reward: 15.80                                                             \n",
      "Episode 58 complete | Score: 51 | Steps: 33 | Reward: 18.20                                                             \n",
      "Episode 59 complete | Score: 48 | Steps: 33 | Reward: 16.70                                                             \n",
      "                                                                                                                        \n",
      "Episode 60/1000 (1x parallel)\n",
      "  Avg Reward: 17.17 | Avg Score: 49 | Avg Steps: 33\n",
      "  Loss: 0.4770 | Epsilon: 0.824 | Memory: 1960/10000 | GPU: 49MB\n",
      "  Best Score: 75 | Best Reward: 31.20\n",
      "  Improvement (last 10 eps): Reward -0.55 | Score -0.70\n",
      "--------------------------------------------------------------------------------\n",
      "Episode 61 complete | Score: 47 | Steps: 32 | Reward: 16.10                                                             \n",
      "Episode 62 complete | Score: 48 | Steps: 32 | Reward: 16.60                                                             \n",
      "Episode 63 complete | Score: 48 | Steps: 33 | Reward: 16.20                                                             \n",
      "Episode 64 complete | Score: 63 | Steps: 41 | Reward: 25.50                                                             \n",
      "Episode 65 complete | Score: 58 | Steps: 40 | Reward: 22.90 | Œî Reward: +0.84, Œî Score: +1.50                           \n",
      "Episode 66 complete | Score: 48 | Steps: 31 | Reward: 17.00                                                             \n",
      "Episode 67 complete | Score: 48 | Steps: 30 | Reward: 16.40                                                             \n",
      "Episode 68 complete | Score: 48 | Steps: 31 | Reward: 16.00                                                             \n",
      "Episode 69 complete | Score: 51 | Steps: 34 | Reward: 18.80                                                             \n",
      "                                                                                                                        \n",
      "Episode 70/1000 (1x parallel)\n",
      "  Avg Reward: 18.38 | Avg Score: 51 | Avg Steps: 34\n",
      "  Loss: 0.9486 | Epsilon: 0.793 | Memory: 2298/10000 | GPU: 49MB\n",
      "  Best Score: 75 | Best Reward: 31.20\n",
      "  Improvement (last 10 eps): Reward +1.21 | Score +1.80\n",
      "--------------------------------------------------------------------------------\n",
      "Episode 71 complete | Score: 48 | Steps: 30 | Reward: 16.40                                                             \n",
      "Episode 72 complete | Score: 55 | Steps: 34 | Reward: 19.80                                                             \n",
      "Episode 73 complete | Score: 48 | Steps: 31 | Reward: 16.50                                                             \n",
      "Episode 74 complete | Score: 61 | Steps: 39 | Reward: 24.30                                                             \n",
      "Episode 75 complete | Score: 48 | Steps: 30 | Reward: 16.40 | Œî Reward: -0.19, Œî Score: -0.20                           \n",
      "Episode 76 complete | Score: 50 | Steps: 31 | Reward: 17.00                                                             \n",
      "Episode 77 complete | Score: 47 | Steps: 31 | Reward: 16.50                                                             \n",
      "Episode 78 complete | Score: 61 | Steps: 37 | Reward: 23.60                                                             \n",
      "Episode 79 complete | Score: 60 | Steps: 39 | Reward: 23.30                                                             \n",
      "                                                                                                                        \n",
      "Episode 80/1000 (1x parallel)\n",
      "  Avg Reward: 19.02 | Avg Score: 53 | Avg Steps: 33\n",
      "  Loss: 0.8504 | Epsilon: 0.763 | Memory: 2630/10000 | GPU: 49MB\n",
      "  Best Score: 75 | Best Reward: 31.20\n",
      "  Improvement (last 10 eps): Reward +0.64 | Score +1.60\n",
      "--------------------------------------------------------------------------------\n",
      "Episode 81 complete | Score: 48 | Steps: 31 | Reward: 16.50                                                             \n",
      "Episode 82 complete | Score: 60 | Steps: 40 | Reward: 23.40                                                             \n",
      "Episode 83 complete | Score: 48 | Steps: 33 | Reward: 16.70                                                             \n",
      "Episode 84 complete | Score: 48 | Steps: 34 | Reward: 16.80                                                             \n",
      "Episode 85 complete | Score: 55 | Steps: 34 | Reward: 19.80 | Œî Reward: +1.01, Œî Score: +1.90                           \n",
      "Episode 86 complete | Score: 48 | Steps: 32 | Reward: 16.60                                                             \n",
      "Episode 87 complete | Score: 47 | Steps: 33 | Reward: 16.70                                                             \n",
      "Episode 88 complete | Score: 47 | Steps: 30 | Reward: 16.40                                                             \n",
      "Episode 89 complete | Score: 62 | Steps: 40 | Reward: 23.90                                                             \n",
      "                                                                                                                        \n",
      "Episode 90/1000 (1x parallel)\n",
      "  Avg Reward: 18.56 | Avg Score: 51 | Avg Steps: 34\n",
      "  Loss: 0.6260 | Epsilon: 0.733 | Memory: 2971/10000 | GPU: 49MB\n",
      "  Best Score: 75 | Best Reward: 31.20\n",
      "  Improvement (last 10 eps): Reward -0.46 | Score -1.20\n",
      "--------------------------------------------------------------------------------\n",
      "Episode 91 complete | Score: 47 | Steps: 28 | Reward: 15.70                                                             \n",
      "Episode 92 complete | Score: 48 | Steps: 30 | Reward: 16.40                                                             \n",
      "Episode 93 complete | Score: 48 | Steps: 30 | Reward: 16.40                                                             \n",
      "Episode 94 complete | Score: 48 | Steps: 30 | Reward: 16.40                                                             \n",
      "Episode 95 complete | Score: 58 | Steps: 38 | Reward: 22.20 | Œî Reward: -1.05, Œî Score: -2.10                           \n",
      "Episode 96 complete | Score: 51 | Steps: 34 | Reward: 17.80                                                             \n",
      "Episode 97 complete | Score: 50 | Steps: 35 | Reward: 17.90                                                             \n",
      "Episode 98 complete | Score: 47 | Steps: 30 | Reward: 16.40                                                             \n",
      "‚úì Model saved to models/best_model.pth                                                                                  \n",
      "üèÜ New best score: 77\n",
      "Episode 99 complete | Score: 77 | Steps: 52 | Reward: 33.60\n",
      "                                                                                                                        \n",
      "Episode 100/1000 (1x parallel)\n",
      "  Avg Reward: 18.86 | Avg Score: 52 | Avg Steps: 34\n",
      "  Loss: 1.3541 | Epsilon: 0.702 | Memory: 3307/10000 | GPU: 49MB\n",
      "  Best Score: 77 | Best Reward: 33.60\n",
      "  Improvement (last 10 eps): Reward +0.30 | Score +0.80\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì Model saved to models/checkpoint_ep100.pth\n",
      "üíæ Checkpoint saved: models/checkpoint_ep100.pth\n",
      "Episode 101 complete | Score: 50 | Steps: 34 | Reward: 18.30                                                            \n",
      "Episode 102 complete | Score: 48 | Steps: 29 | Reward: 16.30                                                            \n",
      "Episode 103 complete | Score: 48 | Steps: 32 | Reward: 16.60                                                            \n",
      "Episode 104 complete | Score: 51 | Steps: 34 | Reward: 18.30                                                            \n",
      "Episode 105 complete | Score: 48 | Steps: 32 | Reward: 16.60 | Œî Reward: +0.81, Œî Score: +1.40                          \n",
      "Episode 106 complete | Score: 48 | Steps: 31 | Reward: 17.00                                                            \n",
      "Episode 107 complete | Score: 49 | Steps: 30 | Reward: 16.90                                                            \n",
      "Episode 108 complete | Score: 49 | Steps: 29 | Reward: 16.80                                                            \n",
      "Episode 109 complete | Score: 48 | Steps: 30 | Reward: 16.40                                                            \n",
      "                                                                                                                        \n",
      "Episode 110/1000 (1x parallel)\n",
      "  Avg Reward: 17.01 | Avg Score: 49 | Avg Steps: 31\n",
      "  Loss: 0.8759 | Epsilon: 0.674 | Memory: 3618/10000 | GPU: 49MB\n",
      "  Best Score: 77 | Best Reward: 33.60\n",
      "  Improvement (last 10 eps): Reward -1.85 | Score -3.40\n",
      "--------------------------------------------------------------------------------\n",
      "Episode 111 complete | Score: 48 | Steps: 32 | Reward: 16.60                                                            \n",
      "Episode 112 complete | Score: 55 | Steps: 37 | Reward: 21.10                                                            \n",
      "Episode 113 complete | Score: 48 | Steps: 33 | Reward: 16.70                                                            \n",
      "Episode 114 complete | Score: 58 | Steps: 36 | Reward: 22.50                                                            \n",
      "Episode 115 complete | Score: 47 | Steps: 34 | Reward: 16.80 | Œî Reward: -0.99, Œî Score: -1.90                          \n",
      "Episode 116 complete | Score: 51 | Steps: 32 | Reward: 18.10                                                            \n",
      "Episode 117 complete | Score: 48 | Steps: 31 | Reward: 16.50                                                            \n",
      "Episode 118 complete | Score: 49 | Steps: 30 | Reward: 16.90                                                            \n",
      "Episode 119 complete | Score: 59 | Steps: 35 | Reward: 21.90                                                            \n",
      "                                                                                                                        \n",
      "Episode 120/1000 (1x parallel)\n",
      "  Avg Reward: 18.82 | Avg Score: 52 | Avg Steps: 34\n",
      "  Loss: 0.7000 | Epsilon: 0.644 | Memory: 3955/10000 | GPU: 49MB\n",
      "  Best Score: 77 | Best Reward: 33.60\n",
      "  Improvement (last 10 eps): Reward +1.81 | Score +3.00\n",
      "--------------------------------------------------------------------------------\n",
      "Episode 121 complete | Score: 48 | Steps: 32 | Reward: 16.60                                                            \n",
      "Episode 122 complete | Score: 51 | Steps: 34 | Reward: 18.30                                                            \n",
      "Episode 123 complete | Score: 52 | Steps: 35 | Reward: 18.40                                                            \n",
      "Episode 124 complete | Score: 60 | Steps: 40 | Reward: 23.40                                                            \n",
      "Episode 125 complete | Score: 58 | Steps: 38 | Reward: 22.20 | Œî Reward: +1.57, Œî Score: +3.20                          \n",
      "Episode 126 complete | Score: 48 | Steps: 32 | Reward: 16.60                                                            \n",
      "Episode 127 complete | Score: 47 | Steps: 30 | Reward: 16.40                                                            \n",
      "Episode 128 complete | Score: 48 | Steps: 33 | Reward: 16.70                                                            \n",
      "Episode 129 complete | Score: 50 | Steps: 34 | Reward: 18.30                                                            \n",
      "                                                                                                                        \n",
      "Episode 130/1000 (1x parallel)\n",
      "  Avg Reward: 18.30 | Avg Score: 51 | Avg Steps: 34\n",
      "  Loss: 1.0554 | Epsilon: 0.613 | Memory: 4295/10000 | GPU: 49MB\n",
      "  Best Score: 77 | Best Reward: 33.60\n",
      "  Improvement (last 10 eps): Reward -0.52 | Score -0.90\n",
      "--------------------------------------------------------------------------------\n",
      "Episode 131 complete | Score: 69 | Steps: 42 | Reward: 28.10                                                            \n",
      "Episode 132 complete | Score: 47 | Steps: 31 | Reward: 16.50                                                            \n",
      "Episode 133 complete | Score: 48 | Steps: 32 | Reward: 16.60                                                            \n",
      "Episode 134 complete | Score: 49 | Steps: 33 | Reward: 17.20                                                            \n",
      "Episode 135 complete | Score: 49 | Steps: 30 | Reward: 16.90 | Œî Reward: -1.40, Œî Score: -2.90                          \n",
      "Episode 136 complete | Score: 48 | Steps: 31 | Reward: 17.00                                                            \n",
      "Episode 137 complete | Score: 47 | Steps: 33 | Reward: 16.70                                                            \n",
      "Episode 138 complete | Score: 48 | Steps: 31 | Reward: 16.00                                                            \n",
      "‚úì Model saved to models/best_model.pth                                                                                  \n",
      "üèÜ New best score: 80\n",
      "Episode 139 complete | Score: 80 | Steps: 49 | Reward: 34.30\n",
      "                                                                                                                        \n",
      "Episode 140/1000 (1x parallel)\n",
      "  Avg Reward: 19.59 | Avg Score: 53 | Avg Steps: 34\n",
      "  Loss: 0.8969 | Epsilon: 0.582 | Memory: 4639/10000 | GPU: 49MB\n",
      "  Best Score: 80 | Best Reward: 34.30\n",
      "  Improvement (last 10 eps): Reward +1.29 | Score +2.40\n",
      "--------------------------------------------------------------------------------\n",
      "Episode 141 complete | Score: 48 | Steps: 29 | Reward: 16.30                                                            \n",
      "Episode 142 complete | Score: 50 | Steps: 32 | Reward: 17.60                                                            \n",
      "Episode 143 complete | Score: 51 | Steps: 34 | Reward: 18.30                                                            \n",
      "Episode 144 complete | Score: 47 | Steps: 30 | Reward: 16.40                                                            \n",
      "Episode 145 complete | Score: 51 | Steps: 32 | Reward: 17.60 | Œî Reward: +0.74, Œî Score: +1.60                          \n",
      "Episode 146 complete | Score: 51 | Steps: 34 | Reward: 18.30                                                            \n",
      "Episode 147 complete | Score: 48 | Steps: 31 | Reward: 16.50                                                            \n",
      "Episode 148 complete | Score: 50 | Steps: 32 | Reward: 18.10                                                            \n",
      "Episode 149 complete | Score: 48 | Steps: 29 | Reward: 15.80                                                            \n",
      "                                                                                                                        \n",
      "Episode 150/1000 (1x parallel)\n",
      "  Avg Reward: 17.31 | Avg Score: 49 | Avg Steps: 32\n",
      "  Loss: 0.8378 | Epsilon: 0.554 | Memory: 4955/10000 | GPU: 49MB\n",
      "  Best Score: 80 | Best Reward: 34.30\n",
      "  Improvement (last 10 eps): Reward -2.28 | Score -3.90\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì Model saved to models/checkpoint_ep150.pth\n",
      "üíæ Checkpoint saved: models/checkpoint_ep150.pth\n",
      "Episode 151 complete | Score: 49 | Steps: 32 | Reward: 17.10                                                            \n",
      "Episode 152 complete | Score: 49 | Steps: 33 | Reward: 17.20                                                            \n",
      "Episode 153 complete | Score: 47 | Steps: 29 | Reward: 16.30                                                            \n",
      "Episode 154 complete | Score: 47 | Steps: 31 | Reward: 16.00                                                            \n",
      "Episode 155 complete | Score: 47 | Steps: 33 | Reward: 16.70 | Œî Reward: -1.66, Œî Score: -3.20                          \n",
      "Episode 156 complete | Score: 48 | Steps: 31 | Reward: 17.00                                                            \n",
      "Episode 157 complete | Score: 47 | Steps: 32 | Reward: 16.10                                                            \n",
      "Episode 158 complete | Score: 48 | Steps: 30 | Reward: 16.90                                                            \n",
      "Episode 159 complete | Score: 47 | Steps: 30 | Reward: 16.40                                                            \n",
      "                                                                                                                        \n",
      "Episode 160/1000 (1x parallel)\n",
      "  Avg Reward: 16.73 | Avg Score: 48 | Avg Steps: 31\n",
      "  Loss: 1.0911 | Epsilon: 0.526 | Memory: 5268/10000 | GPU: 49MB\n",
      "  Best Score: 80 | Best Reward: 34.30\n",
      "  Improvement (last 10 eps): Reward -0.58 | Score -1.50\n",
      "--------------------------------------------------------------------------------\n",
      "Episode 161 complete | Score: 50 | Steps: 31 | Reward: 17.50                                                            \n",
      "Episode 162 complete | Score: 69 | Steps: 43 | Reward: 27.70                                                            \n",
      "Episode 163 complete | Score: 59 | Steps: 37 | Reward: 22.60                                                            \n",
      "Episode 164 complete | Score: 48 | Steps: 31 | Reward: 16.00                                                            \n",
      "Episode 165 complete | Score: 47 | Steps: 31 | Reward: 16.00 | Œî Reward: +1.36, Œî Score: +2.70                          \n",
      "Episode 166 complete | Score: 61 | Steps: 41 | Reward: 24.50                                                            \n",
      "Episode 167 complete | Score: 48 | Steps: 32 | Reward: 16.60                                                            \n",
      "Episode 168 complete | Score: 51 | Steps: 32 | Reward: 18.10                                                            \n",
      "Episode 169 complete | Score: 48 | Steps: 31 | Reward: 16.50                                                            \n",
      "                                                                                                                        \n",
      "Episode 170/1000 (1x parallel)\n",
      "  Avg Reward: 20.83 | Avg Score: 56 | Avg Steps: 36\n",
      "  Loss: 1.1007 | Epsilon: 0.494 | Memory: 5626/10000 | GPU: 49MB\n",
      "  Best Score: 80 | Best Reward: 34.30\n",
      "  Improvement (last 10 eps): Reward +4.10 | Score +7.90\n",
      "--------------------------------------------------------------------------------\n",
      "Episode 171 complete | Score: 51 | Steps: 34 | Reward: 18.80                                                            \n",
      "Episode 172 complete | Score: 48 | Steps: 31 | Reward: 16.50                                                            \n",
      "Episode 173 complete | Score: 47 | Steps: 32 | Reward: 16.10                                                            \n",
      "Episode 174 complete | Score: 47 | Steps: 31 | Reward: 16.50                                                            \n",
      "Episode 175 complete | Score: 48 | Steps: 29 | Reward: 16.80 | Œî Reward: +0.94, Œî Score: +1.30                          \n",
      "Episode 176 complete | Score: 48 | Steps: 31 | Reward: 16.50                                                            \n",
      "Episode 177 complete | Score: 48 | Steps: 29 | Reward: 15.80                                                            \n",
      "Episode 178 complete | Score: 48 | Steps: 32 | Reward: 16.60                                                            \n",
      "Episode 179 complete | Score: 60 | Steps: 40 | Reward: 23.40                                                            \n",
      "                                                                                                                        \n",
      "Episode 180/1000 (1x parallel)\n",
      "  Avg Reward: 17.56 | Avg Score: 50 | Avg Steps: 32\n",
      "  Loss: 0.8621 | Epsilon: 0.465 | Memory: 5947/10000 | GPU: 49MB\n",
      "  Best Score: 80 | Best Reward: 34.30\n",
      "  Improvement (last 10 eps): Reward -3.27 | Score -6.10\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì Model saved to models/best_model.pth                                                                                  \n",
      "üèÜ New best score: 82\n",
      "Episode 181 complete | Score: 82 | Steps: 50 | Reward: 35.40\n",
      "Episode 182 complete | Score: 51 | Steps: 34 | Reward: 18.30                                                            \n",
      "Episode 183 complete | Score: 48 | Steps: 29 | Reward: 16.30                                                            \n",
      "Episode 184 complete | Score: 56 | Steps: 37 | Reward: 21.10                                                            \n",
      "Episode 185 complete | Score: 56 | Steps: 35 | Reward: 20.90 | Œî Reward: +0.97, Œî Score: +2.30                          \n",
      "Episode 186 complete | Score: 50 | Steps: 33 | Reward: 17.70                                                            \n",
      "Episode 187 complete | Score: 48 | Steps: 29 | Reward: 16.30                                                            \n",
      "Episode 188 complete | Score: 47 | Steps: 30 | Reward: 16.40                                                            \n",
      "Episode 189 complete | Score: 48 | Steps: 32 | Reward: 16.60                                                            \n",
      "                                                                                                                        \n",
      "Episode 190/1000 (1x parallel)\n",
      "  Avg Reward: 19.56 | Avg Score: 53 | Avg Steps: 34\n",
      "  Loss: 1.0800 | Epsilon: 0.434 | Memory: 6288/10000 | GPU: 49MB\n",
      "  Best Score: 82 | Best Reward: 35.40\n",
      "  Improvement (last 10 eps): Reward +2.00 | Score +3.60\n",
      "--------------------------------------------------------------------------------\n",
      "Episode 191 complete | Score: 58 | Steps: 37 | Reward: 22.10                                                            \n",
      "Episode 192 complete | Score: 49 | Steps: 33 | Reward: 17.70                                                            \n",
      "Episode 193 complete | Score: 63 | Steps: 39 | Reward: 25.30                                                            \n",
      "Episode 194 complete | Score: 47 | Steps: 31 | Reward: 16.50                                                            \n",
      "Episode 195 complete | Score: 48 | Steps: 32 | Reward: 17.10 | Œî Reward: -2.06, Œî Score: -4.40                          \n",
      "Episode 196 complete | Score: 47 | Steps: 32 | Reward: 16.60                                                            \n",
      "Episode 197 complete | Score: 50 | Steps: 33 | Reward: 18.20                                                            \n",
      "Episode 198 complete | Score: 47 | Steps: 30 | Reward: 15.90                                                            \n",
      "Episode 199 complete | Score: 47 | Steps: 29 | Reward: 16.30                                                            \n",
      "                                                                                                                        \n",
      "Episode 200/1000 (1x parallel)\n",
      "  Avg Reward: 19.41 | Avg Score: 52 | Avg Steps: 34\n",
      "  Loss: 0.9774 | Epsilon: 0.403 | Memory: 6629/10000 | GPU: 49MB\n",
      "  Best Score: 82 | Best Reward: 35.40\n",
      "  Improvement (last 10 eps): Reward -0.15 | Score -0.80\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì Model saved to models/checkpoint_ep200.pth\n",
      "üíæ Checkpoint saved: models/checkpoint_ep200.pth\n",
      "Episode 201 complete | Score: 49 | Steps: 31 | Reward: 17.50                                                            \n",
      "Episode 202 complete | Score: 48 | Steps: 30 | Reward: 16.40                                                            \n",
      "Episode 203 complete | Score: 47 | Steps: 32 | Reward: 16.10                                                            \n",
      "Episode 204 complete | Score: 51 | Steps: 31 | Reward: 18.00                                                            \n",
      "Episode 205 complete | Score: 48 | Steps: 33 | Reward: 16.70 | Œî Reward: -0.22, Œî Score: -0.20                          \n",
      "Episode 206 complete | Score: 61 | Steps: 38 | Reward: 24.20                                                            \n",
      "Episode 207 complete | Score: 48 | Steps: 31 | Reward: 16.50                                                            \n",
      "Episode 208 complete | Score: 51 | Steps: 31 | Reward: 18.00                                                            \n",
      "Episode 209 complete | Score: 50 | Steps: 30 | Reward: 17.40                                                            \n",
      "                                                                                                                        \n",
      "Episode 210/1000 (1x parallel)\n",
      "  Avg Reward: 17.66 | Avg Score: 50 | Avg Steps: 32\n",
      "  Loss: 1.0300 | Epsilon: 0.375 | Memory: 6945/10000 | GPU: 49MB\n",
      "  Best Score: 82 | Best Reward: 35.40\n",
      "  Improvement (last 10 eps): Reward -1.75 | Score -2.40\n",
      "--------------------------------------------------------------------------------\n",
      "Episode 211 complete | Score: 49 | Steps: 30 | Reward: 16.40                                                            \n",
      "Episode 212 complete | Score: 48 | Steps: 29 | Reward: 16.30                                                            \n",
      "Episode 213 complete | Score: 47 | Steps: 31 | Reward: 16.50                                                            \n",
      "Episode 214 complete | Score: 48 | Steps: 31 | Reward: 16.50                                                            \n",
      "Episode 215 complete | Score: 47 | Steps: 32 | Reward: 16.10 | Œî Reward: -0.64, Œî Score: -0.60                          \n",
      "Episode 216 complete | Score: 48 | Steps: 30 | Reward: 16.40                                                            \n",
      "Episode 217 complete | Score: 47 | Steps: 29 | Reward: 15.80                                                            \n",
      "Episode 218 complete | Score: 48 | Steps: 29 | Reward: 16.30                                                            \n",
      "Episode 219 complete | Score: 51 | Steps: 31 | Reward: 17.50                                                            \n",
      "                                                                                                                        \n",
      "Episode 220/1000 (1x parallel)\n",
      "  Avg Reward: 16.61 | Avg Score: 48 | Avg Steps: 31\n",
      "  Loss: 1.1913 | Epsilon: 0.347 | Memory: 7251/10000 | GPU: 49MB\n",
      "  Best Score: 82 | Best Reward: 35.40\n",
      "  Improvement (last 10 eps): Reward -1.05 | Score -1.70\n",
      "--------------------------------------------------------------------------------\n",
      "Episode 221 complete | Score: 48 | Steps: 31 | Reward: 16.50                                                            \n",
      "Episode 222 complete | Score: 47 | Steps: 28 | Reward: 16.20                                                            \n",
      "Episode 223 complete | Score: 61 | Steps: 37 | Reward: 23.60                                                            \n",
      "Episode 224 complete | Score: 48 | Steps: 31 | Reward: 16.50                                                            \n",
      "Episode 225 complete | Score: 48 | Steps: 32 | Reward: 16.60 | Œî Reward: +0.00, Œî Score: +0.00                          \n",
      "Episode 226 complete | Score: 55 | Steps: 35 | Reward: 20.40                                                            \n",
      "Episode 227 complete | Score: 49 | Steps: 31 | Reward: 17.50                                                            \n",
      "Episode 228 complete | Score: 50 | Steps: 31 | Reward: 17.50                                                            \n",
      "Episode 229 complete | Score: 48 | Steps: 31 | Reward: 16.50                                                            \n",
      "                                                                                                                        \n",
      "Episode 230/1000 (1x parallel)\n",
      "  Avg Reward: 18.99 | Avg Score: 52 | Avg Steps: 33\n",
      "  Loss: 1.0313 | Epsilon: 0.318 | Memory: 7580/10000 | GPU: 49MB\n",
      "  Best Score: 82 | Best Reward: 35.40\n",
      "  Improvement (last 10 eps): Reward +2.38 | Score +4.10\n",
      "--------------------------------------------------------------------------------\n",
      "Episode 231 complete | Score: 51 | Steps: 32 | Reward: 18.60                                                            \n",
      "Episode 232 complete | Score: 48 | Steps: 30 | Reward: 16.40                                                            \n",
      "Episode 233 complete | Score: 51 | Steps: 34 | Reward: 18.30                                                            \n",
      "Episode 234 complete | Score: 55 | Steps: 34 | Reward: 19.80                                                            \n",
      "Episode 235 complete | Score: 48 | Steps: 29 | Reward: 16.30 | Œî Reward: +1.62, Œî Score: +2.90                          \n",
      "Episode 236 complete | Score: 50 | Steps: 32 | Reward: 17.10                                                            \n",
      "Episode 237 complete | Score: 48 | Steps: 30 | Reward: 16.40                                                            \n",
      "Episode 238 complete | Score: 61 | Steps: 37 | Reward: 23.60                                                            \n",
      "Episode 239 complete | Score: 48 | Steps: 30 | Reward: 16.40                                                            \n",
      "                                                                                                                        \n",
      "Episode 240/1000 (1x parallel)\n",
      "  Avg Reward: 17.93 | Avg Score: 51 | Avg Steps: 32\n",
      "  Loss: 0.9764 | Epsilon: 0.289 | Memory: 7898/10000 | GPU: 49MB\n",
      "  Best Score: 82 | Best Reward: 35.40\n",
      "  Improvement (last 10 eps): Reward -1.06 | Score -1.70\n",
      "--------------------------------------------------------------------------------\n",
      "Episode 241 complete | Score: 51 | Steps: 34 | Reward: 17.80                                                            \n",
      "Episode 242 complete | Score: 48 | Steps: 31 | Reward: 16.50                                                            \n",
      "Episode 243 complete | Score: 48 | Steps: 31 | Reward: 16.50                                                            \n",
      "Episode 244 complete | Score: 64 | Steps: 40 | Reward: 25.40                                                            \n",
      "Episode 245 complete | Score: 51 | Steps: 34 | Reward: 18.30 | Œî Reward: -0.55, Œî Score: -0.90                          \n",
      "Episode 246 complete | Score: 48 | Steps: 31 | Reward: 16.00                                                            \n",
      "Episode 247 complete | Score: 50 | Steps: 30 | Reward: 17.90                                                            \n",
      "Episode 248 complete | Score: 48 | Steps: 31 | Reward: 16.50                                                            \n",
      "Episode 249 complete | Score: 70 | Steps: 47 | Reward: 29.10                                                            \n",
      "                                                                                                                        \n",
      "Episode 250/1000 (1x parallel)\n",
      "  Avg Reward: 19.24 | Avg Score: 53 | Avg Steps: 34\n",
      "  Loss: 1.1685 | Epsilon: 0.258 | Memory: 8242/10000 | GPU: 49MB\n",
      "  Best Score: 82 | Best Reward: 35.40\n",
      "  Improvement (last 10 eps): Reward +1.31 | Score +2.00\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì Model saved to models/checkpoint_ep250.pth\n",
      "üíæ Checkpoint saved: models/checkpoint_ep250.pth\n",
      "Episode 251 complete | Score: 50 | Steps: 35 | Reward: 17.90                                                            \n",
      "Episode 252 complete | Score: 48 | Steps: 31 | Reward: 16.00                                                            \n",
      "Episode 253 complete | Score: 48 | Steps: 34 | Reward: 16.80                                                            \n",
      "Episode 254 complete | Score: 47 | Steps: 30 | Reward: 16.40                                                            \n",
      "Episode 255 complete | Score: 48 | Steps: 30 | Reward: 15.90 | Œî Reward: -0.35, Œî Score: -1.00                          \n",
      "Episode 256 complete | Score: 50 | Steps: 34 | Reward: 17.80                                                            \n",
      "Episode 257 complete | Score: 50 | Steps: 33 | Reward: 17.70                                                            \n",
      "Episode 258 complete | Score: 48 | Steps: 33 | Reward: 17.20                                                            \n",
      "Episode 259 complete | Score: 59 | Steps: 37 | Reward: 22.60                                                            \n",
      "                                                                                                                        \n",
      "Episode 260/1000 (1x parallel)\n",
      "  Avg Reward: 17.50 | Avg Score: 50 | Avg Steps: 33\n",
      "  Loss: 1.2334 | Epsilon: 0.229 | Memory: 8572/10000 | GPU: 49MB\n",
      "  Best Score: 82 | Best Reward: 35.40\n",
      "  Improvement (last 10 eps): Reward -1.74 | Score -3.20\n",
      "--------------------------------------------------------------------------------\n",
      "Episode 261 complete | Score: 48 | Steps: 29 | Reward: 16.30                                                            \n",
      "Episode 262 complete | Score: 48 | Steps: 30 | Reward: 16.40                                                            \n",
      "Episode 263 complete | Score: 51 | Steps: 33 | Reward: 17.70                                                            \n",
      "Episode 264 complete | Score: 48 | Steps: 29 | Reward: 16.30                                                            \n",
      "Episode 265 complete | Score: 50 | Steps: 33 | Reward: 18.20 | Œî Reward: -0.40, Œî Score: -0.70                          \n",
      "Episode 266 complete | Score: 63 | Steps: 42 | Reward: 25.10                                                            \n",
      "Episode 267 complete | Score: 48 | Steps: 31 | Reward: 16.50                                                            \n",
      "Episode 268 complete | Score: 51 | Steps: 33 | Reward: 18.20                                                            \n",
      "Episode 269 complete | Score: 47 | Steps: 28 | Reward: 15.70                                                            \n",
      "                                                                                                                        \n",
      "Episode 270/1000 (1x parallel)\n",
      "  Avg Reward: 17.87 | Avg Score: 50 | Avg Steps: 32\n",
      "  Loss: 1.0499 | Epsilon: 0.200 | Memory: 8894/10000 | GPU: 49MB\n",
      "  Best Score: 82 | Best Reward: 35.40\n",
      "  Improvement (last 10 eps): Reward +0.37 | Score +0.80\n",
      "--------------------------------------------------------------------------------\n",
      "Episode 271 complete | Score: 48 | Steps: 28 | Reward: 16.20                                                            \n",
      "Episode 272 complete | Score: 48 | Steps: 32 | Reward: 16.60                                                            \n",
      "Episode 273 complete | Score: 50 | Steps: 29 | Reward: 16.80                                                            \n",
      "Episode 274 complete | Score: 48 | Steps: 30 | Reward: 15.90                                                            \n",
      "Episode 275 complete | Score: 48 | Steps: 29 | Reward: 15.80 | Œî Reward: -0.18, Œî Score: +0.10                          \n",
      "Episode 276 complete | Score: 57 | Steps: 37 | Reward: 21.60                                                            \n",
      "Episode 277 complete | Score: 49 | Steps: 33 | Reward: 17.20                                                            \n",
      "Episode 278 complete | Score: 59 | Steps: 36 | Reward: 22.00                                                            \n",
      "Episode 279 complete | Score: 47 | Steps: 30 | Reward: 16.40                                                            \n",
      "                                                                                                                        \n",
      "Episode 280/1000 (1x parallel)\n",
      "  Avg Reward: 17.59 | Avg Score: 50 | Avg Steps: 31\n",
      "  Loss: 1.1979 | Epsilon: 0.171 | Memory: 9208/10000 | GPU: 49MB\n",
      "  Best Score: 82 | Best Reward: 35.40\n",
      "  Improvement (last 10 eps): Reward -0.28 | Score +0.10\n",
      "--------------------------------------------------------------------------------\n",
      "Episode 281 complete | Score: 47 | Steps: 29 | Reward: 15.80                                                            \n",
      "Episode 282 complete | Score: 51 | Steps: 34 | Reward: 18.30                                                            \n",
      "Episode 283 complete | Score: 47 | Steps: 30 | Reward: 15.90                                                            \n",
      "Episode 284 complete | Score: 64 | Steps: 42 | Reward: 25.60                                                            \n",
      "Episode 285 complete | Score: 51 | Steps: 31 | Reward: 18.00 | Œî Reward: +1.31, Œî Score: +2.20                          \n",
      "Episode 286 complete | Score: 48 | Steps: 31 | Reward: 16.50                                                            \n",
      "Episode 287 complete | Score: 48 | Steps: 32 | Reward: 16.60                                                            \n",
      "Episode 288 complete | Score: 51 | Steps: 31 | Reward: 18.00                                                            \n",
      "\n",
      "‚ö† Training interrupted by user\n",
      "Saving current progress...\n",
      "‚úì Model saved to models/interrupted_ep289.pth\n",
      "\n",
      "üèÅ Training complete!\n",
      "  Total episodes: 288\n",
      "  Best score: 82\n",
      "  Best reward: 35.40\n",
      "  Parallel speedup: ~1x\n",
      "üìù Full training progress saved to logs/training_progress.txt\n",
      "  Final GPU memory: 49MB\n",
      "‚úì Browser(s) closed\n"
     ]
    }
   ],
   "source": [
    "# Parallel environment tracking\n",
    "num_envs = NUM_PARALLEL_ENVS if NUM_PARALLEL_ENVS > 1 else 1\n",
    "\n",
    "# Calculate starting episode (for resume)\n",
    "start_episode = len(episode_rewards)\n",
    "\n",
    "# Progress tracking file\n",
    "progress_file = 'logs/training_progress.txt'\n",
    "with open(progress_file, 'a') as f:\n",
    "    f.write(f\"\\n{'='*80}\\n\")\n",
    "    f.write(f\"Training session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Starting from episode: {start_episode + 1}\\n\")\n",
    "    f.write(f\"Parallel environments: {num_envs}\\n\")\n",
    "    f.write(f\"{'='*80}\\n\\n\")\n",
    "\n",
    "print(f\"üöÄ Starting training on GPU with {num_envs}x parallel environments...\")\n",
    "if start_episode > 0:\n",
    "    print(f\"üìç Resuming from episode {start_episode + 1}\")\n",
    "print(f\"üìù Progress will be saved to {progress_file}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    for episode in range(start_episode, start_episode + CONFIG['num_episodes']):\n",
    "        # Reset environment(s)\n",
    "        if num_envs > 1:\n",
    "            states, infos = env.reset()\n",
    "            # Convert to PyTorch format: (N, H, W) -> (N, C, H, W)\n",
    "            states = np.expand_dims(states, axis=1)\n",
    "        else:\n",
    "            state, info = env.reset()\n",
    "            states = np.expand_dims(np.expand_dims(state, axis=0), axis=0)\n",
    "            infos = [info]\n",
    "        \n",
    "        episode_rewards_parallel = [0] * num_envs\n",
    "        episode_losses_parallel = []\n",
    "        dones = [False] * num_envs\n",
    "        steps = [0] * num_envs\n",
    "        \n",
    "        # Real-time progress tracking\n",
    "        step_count = 0\n",
    "        \n",
    "        # Episode loop\n",
    "        while not all(dones):\n",
    "            # Select actions for all environments\n",
    "            actions = []\n",
    "            for i in range(num_envs):\n",
    "                if not dones[i]:\n",
    "                    action = agent.act(states[i], training=True)\n",
    "                    actions.append(action)\n",
    "                else:\n",
    "                    actions.append(0)  # Dummy action for done envs\n",
    "            \n",
    "            # Take steps in all environments\n",
    "            if num_envs > 1:\n",
    "                next_states, rewards, terminateds, truncateds, new_infos = env.step(actions)\n",
    "                next_states = np.expand_dims(next_states, axis=1)\n",
    "            else:\n",
    "                next_state, reward, terminated, truncated, new_info = env.step(actions[0])\n",
    "                next_states = np.expand_dims(np.expand_dims(next_state, axis=0), axis=0)\n",
    "                rewards = np.array([reward])\n",
    "                terminateds = np.array([terminated])\n",
    "                truncateds = np.array([truncated])\n",
    "                new_infos = [new_info]\n",
    "            \n",
    "            # Process each environment\n",
    "            for i in range(num_envs):\n",
    "                if not dones[i]:\n",
    "                    done = terminateds[i] or truncateds[i]\n",
    "                    \n",
    "                    # Store experience\n",
    "                    agent.remember(states[i], actions[i], rewards[i], next_states[i], done)\n",
    "                    \n",
    "                    # Train agent\n",
    "                    loss = agent.replay()\n",
    "                    if loss > 0:\n",
    "                        episode_losses_parallel.append(loss)\n",
    "                    \n",
    "                    # Update metrics\n",
    "                    episode_rewards_parallel[i] += rewards[i]\n",
    "                    steps[i] += 1\n",
    "                    agent.steps += 1\n",
    "                    \n",
    "                    # Update target network\n",
    "                    if agent.steps % agent.target_update_freq == 0:\n",
    "                        agent.update_target_network()\n",
    "                    \n",
    "                    # Decay epsilon\n",
    "                    agent.update_epsilon()\n",
    "                    \n",
    "                    # Mark as done\n",
    "                    if done:\n",
    "                        dones[i] = True\n",
    "                        infos[i] = new_infos[i]\n",
    "            \n",
    "            # Update states\n",
    "            states = next_states\n",
    "            step_count += 1\n",
    "            \n",
    "            # Real-time progress update (every 50 steps)\n",
    "            if step_count % 50 == 0:\n",
    "                active_envs = sum(1 for d in dones if not d)\n",
    "                avg_steps = np.mean([s for i, s in enumerate(steps) if not dones[i]]) if active_envs > 0 else np.mean(steps)\n",
    "                current_scores = [info.get('score', 0) for i, info in enumerate(infos) if not dones[i]]\n",
    "                avg_score = np.mean(current_scores) if current_scores else 0\n",
    "                \n",
    "                print(f\"  Episode {episode + 1} | Step {step_count} | \"\n",
    "                      f\"Active: {active_envs}/{num_envs} | \"\n",
    "                      f\"Avg Steps: {avg_steps:.0f} | \"\n",
    "                      f\"Current Score: {avg_score:.0f} | \"\n",
    "                      f\"Œµ: {agent.epsilon:.3f}\", end='\\r')\n",
    "            \n",
    "            # Break if max steps reached for all envs\n",
    "            if all(s >= CONFIG['max_steps_per_episode'] for s in steps):\n",
    "                break\n",
    "        \n",
    "        # Clear the progress line\n",
    "        print(\" \" * 120, end='\\r')\n",
    "        \n",
    "        # Episode complete - record metrics (average across parallel envs)\n",
    "        avg_episode_reward = np.mean(episode_rewards_parallel)\n",
    "        avg_episode_score = np.mean([info.get('score', 0) for info in infos])\n",
    "        avg_episode_length = np.mean(steps)\n",
    "        avg_episode_loss = np.mean(episode_losses_parallel) if episode_losses_parallel else 0\n",
    "        \n",
    "        episode_rewards.append(avg_episode_reward)\n",
    "        episode_scores.append(avg_episode_score)\n",
    "        episode_lengths.append(avg_episode_length)\n",
    "        episode_losses.append(avg_episode_loss)\n",
    "        epsilon_history.append(agent.epsilon)\n",
    "        \n",
    "        # Calculate improvement metrics\n",
    "        recent_window = 10\n",
    "        if len(episode_rewards) >= recent_window:\n",
    "            recent_avg_reward = np.mean(episode_rewards[-recent_window:])\n",
    "            prev_avg_reward = np.mean(episode_rewards[-recent_window*2:-recent_window]) if len(episode_rewards) >= recent_window*2 else np.mean(episode_rewards[:recent_window])\n",
    "            reward_improvement = recent_avg_reward - prev_avg_reward\n",
    "            \n",
    "            recent_avg_score = np.mean(episode_scores[-recent_window:])\n",
    "            prev_avg_score = np.mean(episode_scores[-recent_window*2:-recent_window]) if len(episode_scores) >= recent_window*2 else np.mean(episode_scores[:recent_window])\n",
    "            score_improvement = recent_avg_score - prev_avg_score\n",
    "        else:\n",
    "            reward_improvement = 0\n",
    "            score_improvement = 0\n",
    "        \n",
    "        # Update best performance\n",
    "        max_score = max(info.get('score', 0) for info in infos)\n",
    "        is_new_best = False\n",
    "        if max_score > best_score:\n",
    "            best_score = max_score\n",
    "            agent.save(f'models/best_model.pth')\n",
    "            is_new_best = True\n",
    "            print(f\"üèÜ New best score: {best_score:.0f}\")\n",
    "        \n",
    "        if avg_episode_reward > best_reward:\n",
    "            best_reward = avg_episode_reward\n",
    "        \n",
    "        # Save detailed progress to file\n",
    "        with open(progress_file, 'a') as f:\n",
    "            f.write(f\"Episode {episode + 1}/{start_episode + CONFIG['num_episodes']} - {datetime.now().strftime('%H:%M:%S')}\\n\")\n",
    "            f.write(f\"  Score: {avg_episode_score:.2f} (Best: {best_score:.0f}{'  üèÜ NEW BEST!' if is_new_best else ''})\\n\")\n",
    "            f.write(f\"  Reward: {avg_episode_reward:.2f} (Best: {best_reward:.2f})\\n\")\n",
    "            f.write(f\"  Steps: {avg_episode_length:.0f}\\n\")\n",
    "            f.write(f\"  Loss: {avg_episode_loss:.6f}\\n\")\n",
    "            f.write(f\"  Epsilon: {agent.epsilon:.4f}\\n\")\n",
    "            f.write(f\"  Total training steps: {agent.steps}\\n\")\n",
    "            f.write(f\"  Memory buffer: {len(agent.memory)}/{CONFIG['memory_size']}\\n\")\n",
    "            if len(episode_rewards) >= recent_window:\n",
    "                f.write(f\"  Improvement (last {recent_window} eps):\\n\")\n",
    "                f.write(f\"    Reward: {reward_improvement:+.2f}\\n\")\n",
    "                f.write(f\"    Score: {score_improvement:+.2f}\\n\")\n",
    "            f.write(f\"\\n\")\n",
    "        \n",
    "        # Logging\n",
    "        if (episode + 1) % CONFIG['log_interval'] == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-CONFIG['log_interval']:])\n",
    "            avg_score = np.mean(episode_scores[-CONFIG['log_interval']:])\n",
    "            avg_loss = np.mean(episode_losses[-CONFIG['log_interval']:])\n",
    "            avg_length = np.mean(episode_lengths[-CONFIG['log_interval']:])\n",
    "            \n",
    "            # GPU memory info\n",
    "            gpu_mem = \"\"\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_mem = f\" | GPU: {torch.cuda.memory_allocated(0) / 1024**2:.0f}MB\"\n",
    "            \n",
    "            print(f\"\\nEpisode {episode + 1}/{start_episode + CONFIG['num_episodes']} ({num_envs}x parallel)\")\n",
    "            print(f\"  Avg Reward: {avg_reward:.2f} | Avg Score: {avg_score:.0f} | \"\n",
    "                  f\"Avg Steps: {avg_length:.0f}\")\n",
    "            print(f\"  Loss: {avg_loss:.4f} | Epsilon: {agent.epsilon:.3f} | \"\n",
    "                  f\"Memory: {len(agent.memory)}/{CONFIG['memory_size']}{gpu_mem}\")\n",
    "            print(f\"  Best Score: {best_score:.0f} | Best Reward: {best_reward:.2f}\")\n",
    "            \n",
    "            if len(episode_rewards) >= recent_window:\n",
    "                print(f\"  Improvement (last {recent_window} eps): \"\n",
    "                      f\"Reward {reward_improvement:+.2f} | Score {score_improvement:+.2f}\")\n",
    "            print(\"-\" * 80)\n",
    "        else:\n",
    "            # Brief update for non-logged episodes\n",
    "            improvement_str = \"\"\n",
    "            if len(episode_rewards) >= recent_window and (episode + 1) % 5 == 0:\n",
    "                improvement_str = f\" | Œî Reward: {reward_improvement:+.2f}, Œî Score: {score_improvement:+.2f}\"\n",
    "            \n",
    "            print(f\"Episode {episode + 1} complete | Score: {avg_episode_score:.0f} | \"\n",
    "                  f\"Steps: {avg_episode_length:.0f} | \"\n",
    "                  f\"Reward: {avg_episode_reward:.2f}{improvement_str}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (episode + 1) % CONFIG['save_interval'] == 0:\n",
    "            agent.save(f'models/checkpoint_ep{episode + 1}.pth')\n",
    "            \n",
    "            # Save metrics with improvement tracking\n",
    "            metrics = {\n",
    "                'episode': episode + 1,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'num_parallel_envs': num_envs,\n",
    "                'episode_rewards': episode_rewards,\n",
    "                'episode_scores': episode_scores,\n",
    "                'episode_losses': episode_losses,\n",
    "                'episode_lengths': episode_lengths,\n",
    "                'epsilon_history': epsilon_history,\n",
    "                'best_score': best_score,\n",
    "                'best_reward': best_reward,\n",
    "                'total_steps': agent.steps,\n",
    "                'memory_size': len(agent.memory),\n",
    "                'config': CONFIG\n",
    "            }\n",
    "            \n",
    "            # Add improvement metrics\n",
    "            if len(episode_rewards) >= recent_window:\n",
    "                metrics['recent_improvement'] = {\n",
    "                    'window_size': recent_window,\n",
    "                    'reward_improvement': float(reward_improvement),\n",
    "                    'score_improvement': float(score_improvement),\n",
    "                    'recent_avg_reward': float(recent_avg_reward),\n",
    "                    'recent_avg_score': float(recent_avg_score)\n",
    "                }\n",
    "            \n",
    "            with open(f'logs/metrics_ep{episode + 1}.json', 'w') as f:\n",
    "                json.dump(metrics, f, indent=2)\n",
    "            \n",
    "            print(f\"üíæ Checkpoint saved: models/checkpoint_ep{episode + 1}.pth\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö† Training interrupted by user\")\n",
    "    print(\"Saving current progress...\")\n",
    "    agent.save(f'models/interrupted_ep{episode + 1}.pth')\n",
    "    \n",
    "    with open(progress_file, 'a') as f:\n",
    "        f.write(f\"\\n‚ö† Training interrupted at episode {episode + 1}\\n\")\n",
    "        f.write(f\"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "finally:\n",
    "    print(\"\\nüèÅ Training complete!\")\n",
    "    print(f\"  Total episodes: {len(episode_rewards)}\")\n",
    "    print(f\"  Best score: {best_score}\")\n",
    "    print(f\"  Best reward: {best_reward:.2f}\")\n",
    "    print(f\"  Parallel speedup: ~{num_envs}x\")\n",
    "    \n",
    "    # Save final summary to progress file\n",
    "    with open(progress_file, 'a') as f:\n",
    "        f.write(f\"\\n{'='*80}\\n\")\n",
    "        f.write(f\"Training session ended: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Total episodes completed: {len(episode_rewards)}\\n\")\n",
    "        f.write(f\"Best score achieved: {best_score:.0f}\\n\")\n",
    "        f.write(f\"Best reward achieved: {best_reward:.2f}\\n\")\n",
    "        f.write(f\"Total training steps: {agent.steps}\\n\")\n",
    "        f.write(f\"Final epsilon: {agent.epsilon:.4f}\\n\")\n",
    "        f.write(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"üìù Full training progress saved to {progress_file}\")\n",
    "    \n",
    "    # GPU memory cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  Final GPU memory: {torch.cuda.memory_allocated(0) / 1024**2:.0f}MB\")\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Close environment(s)\n",
    "    env.close()\n",
    "    print(\"‚úì Browser(s) closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a49e4",
   "metadata": {},
   "source": [
    "## 9. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55176299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('DQN Training Metrics', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Rewards\n",
    "axes[0, 0].plot(episode_rewards, alpha=0.3, label='Episode Reward')\n",
    "axes[0, 0].plot(np.convolve(episode_rewards, np.ones(50)/50, mode='valid'), \n",
    "                label='Moving Avg (50)', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Total Reward')\n",
    "axes[0, 0].set_title('Episode Rewards')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scores\n",
    "axes[0, 1].plot(episode_scores, alpha=0.3, label='Episode Score')\n",
    "axes[0, 1].plot(np.convolve(episode_scores, np.ones(50)/50, mode='valid'),\n",
    "                label='Moving Avg (50)', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Score (Distance)')\n",
    "axes[0, 1].set_title('Game Scores')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1, 0].plot(episode_losses, alpha=0.6)\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].set_title('Training Loss')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Epsilon\n",
    "axes[1, 1].plot(epsilon_history, color='orange')\n",
    "axes[1, 1].set_xlabel('Episode')\n",
    "axes[1, 1].set_ylabel('Epsilon')\n",
    "axes[1, 1].set_title('Exploration Rate (Epsilon)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/training_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Training plots saved to plots/training_metrics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5bf118",
   "metadata": {},
   "source": [
    "## 10. Test Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d9889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the best model\n",
    "print(\"üéÆ Testing trained agent...\\n\")\n",
    "\n",
    "# Load best model\n",
    "agent.load('models/best_model.pth')\n",
    "agent.epsilon = 0.0  # No exploration during testing\n",
    "\n",
    "# Run test episodes\n",
    "num_test_episodes = 5\n",
    "test_scores = []\n",
    "test_rewards = []\n",
    "\n",
    "env = BrowserDinoEnv()\n",
    "\n",
    "for episode in range(num_test_episodes):\n",
    "    state, info = env.reset()\n",
    "    state = np.expand_dims(state, axis=0)  # Add channel dimension\n",
    "    \n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "    \n",
    "    while not done and step < 5000:\n",
    "        action = agent.act(state, training=False)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        state = np.expand_dims(next_state, axis=0)\n",
    "        episode_reward += reward\n",
    "        step += 1\n",
    "    \n",
    "    test_scores.append(info.get('score', 0))\n",
    "    test_rewards.append(episode_reward)\n",
    "    \n",
    "    print(f\"Test Episode {episode + 1}: Score={info.get('score', 0):.0f}, \"\n",
    "          f\"Reward={episode_reward:.2f}, Steps={step}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\nüìä Test Results:\")\n",
    "print(f\"  Average Score: {np.mean(test_scores):.0f} ¬± {np.std(test_scores):.0f}\")\n",
    "print(f\"  Average Reward: {np.mean(test_rewards):.2f} ¬± {np.std(test_rewards):.2f}\")\n",
    "print(f\"  Best Score: {max(test_scores):.0f}\")\n",
    "print(f\"  Worst Score: {min(test_scores):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9841c5b2",
   "metadata": {},
   "source": [
    "## 11. Save Final Model and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cfb066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final summary\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "summary = {\n",
    "    'timestamp': timestamp,\n",
    "    'hardware': {\n",
    "        'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',\n",
    "        'vram': f\"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f}GB\" if torch.cuda.is_available() else 'N/A',\n",
    "        'pytorch_version': torch.__version__,\n",
    "        'cuda_version': torch.version.cuda if torch.cuda.is_available() else 'N/A'\n",
    "    },\n",
    "    'training': {\n",
    "        'total_episodes': len(episode_rewards),\n",
    "        'total_steps': agent.steps,\n",
    "        'best_score': best_score,\n",
    "        'best_reward': best_reward,\n",
    "        'final_epsilon': agent.epsilon\n",
    "    },\n",
    "    'test_results': {\n",
    "        'num_episodes': num_test_episodes,\n",
    "        'avg_score': float(np.mean(test_scores)),\n",
    "        'avg_reward': float(np.mean(test_rewards)),\n",
    "        'best_test_score': float(max(test_scores))\n",
    "    },\n",
    "    'config': CONFIG\n",
    "}\n",
    "\n",
    "with open(f'logs/training_summary_{timestamp}.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Training summary saved to logs/training_summary_{timestamp}.json\")\n",
    "print(\"\\nüéâ Training pipeline complete!\")\n",
    "print(f\"üöÄ GPU-accelerated training using {summary['hardware']['gpu']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1100cd",
   "metadata": {},
   "source": [
    "## 7.7. Training Analysis & Recommendations\n",
    "\n",
    "‚ö†Ô∏è **ANALYSIS OF EPISODES 1-48 SHOWS NO LEARNING**\n",
    "- Scores stuck at 45-50 (flat line, no improvement)\n",
    "- Only 927 training steps completed (9% of epsilon decay)\n",
    "- Agent behavior suggests reward/environment issues\n",
    "\n",
    "**REQUIRED FIXES BEFORE CONTINUING:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e4d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL FIXES based on log analysis\n",
    "\n",
    "# 1. REDUCE PARALLELISM - Too much noise for initial learning\n",
    "NUM_PARALLEL_ENVS = 1  # Changed from 4 to 1\n",
    "\n",
    "# 2. UPDATE CONFIG - Slower epsilon decay, more frequent target updates\n",
    "CONFIG['epsilon_decay_steps'] = 50000  # Increased from 10,000 (slower exploration decay)\n",
    "CONFIG['target_update_freq'] = 500     # Reduced from 1,000 (more frequent Q-network sync)\n",
    "\n",
    "print(\"‚ö†Ô∏è CONFIGURATION UPDATED FOR BETTER LEARNING:\")\n",
    "print(f\"  Parallel Environments: {NUM_PARALLEL_ENVS} (reduced from 4)\")\n",
    "print(f\"  Epsilon Decay Steps: {CONFIG['epsilon_decay_steps']:,} (increased from 10,000)\")\n",
    "print(f\"  Target Update Freq: {CONFIG['target_update_freq']} (reduced from 1,000)\")\n",
    "print(\"\\nüìù WHY THESE CHANGES:\")\n",
    "print(\"  - Single environment: Clearer learning signal, less variance\")\n",
    "print(\"  - Slower epsilon decay: More exploration before exploitation\")\n",
    "print(\"  - More frequent updates: Better Q-value convergence\")\n",
    "print(\"\\n‚úÖ Now RESTART environments (Section 7.6) and RETRAIN (Section 8)\")\n",
    "print(\"   Expected: Scores should start at 10-30 and gradually improve\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DINO_TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
